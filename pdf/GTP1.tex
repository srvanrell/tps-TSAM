\documentclass[11pt,a4paper,final]{article}

\usepackage[utf8]{inputenc}     % Codificación del archivo fuente en UTF8
\usepackage[T1]{fontenc}        % Incorpora fuente con acentos
\usepackage{amsmath}            % Amplía opciones para las ecuaciones
%\usepackage[margin=2cm]{geometry}

% -- Bibliografia --
%\usepackage[style=ieee,
%            backend=bibtex8,
%            maxcitenames=2,
%            mincitenames=1]
%            {biblatex}          % Control sobre las citas y referencias 
%  \addbibresource{Referencias.bib} % Archivo con las referencias
%\usepackage{url}                % Para incluir url clickeables

% -- Tuneando los captions --
\usepackage[margin=10pt,
            font=small,
            labelfont=bf,
            labelsep=endash]
            {caption}

% -- Tablas --
%\usepackage{booktabs}       % Decorado de tablas
%  \heavyrulewidth=1pt       % Lineas gruesas
%\usepackage{tabu}           % Permite control del ancho relativo entre columnas

% -- Figuras --
\usepackage{graphicx}		    % Permite incluir imágenes
  \graphicspath{{Figuras/}}	    % Ruta relativa donde buscar las imágenes
\usepackage[font=small,
            labelfont=bf, 
            subrefformat=parens]
            {subcaption}
            
% -- Incorporar imágenes de Matlab --
\usepackage{pgfplots}
\pgfplotsset{compat=newest} 
\pgfplotsset{plot coordinates/math parser=false}
\usepackage{tikz}
\usetikzlibrary{plotmarks} % para plotear las graficas que vienen de Matlab
            
%\usepackage{color}
%
%\sloppy
\definecolor{lightgray}{gray}{0.8} % para la grafica 2.3

%---------------------------------------------------------------------------------
% DEPRECATED COMMANDS
%---------------------------------------------------------------------------------  
\usepackage[spanish]{babel}     % Configura en modo español a muchas cosas
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[babel,
            spanish=spanish]
            {csquotes}          % Comillas francesas en la bibliografia
%---------------------------------------------------------------------------------



\author{Sebastián R. Vanrell\\[3em]}
\title{{\large Curso:}\\\medskip
       {\Large Tópicos Selectos en Aprendizaje Maquinal}\\[3em]
       \textsf{Guía de Trabajos Prácticos Nº1}\\\bigskip
       \textsf{Algoritmos para Reconocimiento de Patrones}\\[3em]}
\date{Doctorado en Ingeniería\\\bigskip
      Facultad de Ingeniería y Ciencias Hídricas\\\bigskip
      Universidad Nacional del Litoral \\[5em]
      \today}

\usepackage{color}
\definecolor{lightgray}{gray}{0.5}
\setlength{\parindent}{0pt}

\begin{document}
\renewcommand{\tablename}{Tabla}

\maketitle
\newpage

\tableofcontents

\newpage


\section{Ejercicio 1 - Introducción y repaso de probabilidad y teoría de información}


Escriba un programa para generar números aleatorios $\mathbf{x}$ con una función de densidad de probabilidad ($fdp$) gaussiana $\mathcal{N}(\mathbf{\mu},\mathbf{\Sigma}))$ en $d$ dimensiones.


\subsection{Ejercicio 1.1}

Para cada uno de los siguientes casos: i) $fdp$ gaussiana unidimensional, ii) $fdp$ gaussiana bidimensional no correlacionada y iii) $fdp$ gaussiana bidimensional correlacionada,

\begin{itemize}
   \item[a)] Genere un conjunto de números aleatorios con esa $fdp$.
   \item[b)] Estime la $fdp$ experimental mediante un histrograma normalizado.
   \item[c)] Compare gráficamente la estimación obtenida con la $fdp$ teórica.
\end{itemize}


\subsection*{1.1.i) $fdp$ gaussiana unidimensional}

\begin{verbatim}
N      = 5000;  %
media  = 3;     % definición de la fdp
desvio = 2;     %

x = randgauss1D(media, desvio, N);  % conjunto de números aleatorios

[alturas, centros] = hist(x, 40);   % histograma sin normalizar
ancho = centros(2) - centros(1);    % ancho de las barras del histograma
area  = sum(ancho .* alturas);      % area de las barras del histograma
fdpexp = alturas/area;              % fdp experimental
bar(centros, fdpexp,'w');           % dibuja histograma normalizado

% Comparación con la fdp teórica
t = -7:0.01:13;
fdpteor = normpdf(t,media,desvio); % fdp teórica con igual media y desvio
hold on; plot(t,fdpteor,'-k','LineWidth',2); hold off;

title({'fdp de una distribucion gaussiana unidimensional';
       sprintf('mu = %0.1f, sigma = %0.1f', media, desvio)})
ylabel('p(x)'); xlabel('x');
legend('fdp experimental','fdp teorica')
\end{verbatim}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Ejercicio1_01.png}
	\caption{Comparación fdp teórica y experimental.}
	\label{ejercicio11}
\end{figure}

Para normalizar el histograma se dividió por la suma de las áreas de los rectángulos dado que la integral de una $fdp$ debe dar 1, aún en este caso donde la $fdp$ es empírica. En la comparación gráfica se verifica la correspondencia con la $fdp$ teórica.


Debajo se muestra el código de las dos funciones utilizadas para generar números aleatorios de una distribución normal unidimensional.



\subsubsection*{randgauss.m}

\begin{verbatim}
1     function x = randgauss()
2     %RANDGAUSS() genera un número pseudo aleatorio de una distribución 
3     % gaussiana estándar. 
4     %  El número x es generado por el método polar de Box-Muller a partir de 
5     %  dos números pseudo aleatorios independentes y uniformemente 
6     %  distribuidos en un intervalo cerrado [-1, +1]
7     %  Referencias: 
8     %  - Box, G. E. P. and Muller, M. E. ''A Note on the Generation of Random 
9     %    Normal Deviates.'' Ann. Math. Stat. 29, 610-611, 1958.
10    %  - Bell, J.,  "Algorithm 334: Normal random deviates", Communications 
11    %    of the ACM, vol. 11, No. 7. 1968
12    
13    s=1; % condición para ingresar al while
14    while (s >= 1) || (s == 0)
15        % u1 y u2 independientes y distribuidos uniformemente en [-1, +1]
16        u1 = 2.0 * rand() - 1.0;
17        u2 = 2.0 * rand() - 1.0;
18    	
19        s = u1 .* u1 + u2 .* u2; % variable a evaluar
20    end
21    % a la salida s está uniformemente distribuido en (0, +1)
22    
23    % número aleatorio de una distribución normal estándar
24    x = u1 .* sqrt( (-2.0 .* log( s ) ) ./ s ); 
25    
26    end
\end{verbatim}
    

\subsubsection*{randgauss1D.m}

\begin{verbatim}
1     function x = randgauss1D(mu, sigma, N)
2     %RANDGAUSS1D(mu, sigma, N) genera un vector de números aleatorios con 
3     %distribución gaussiana de media mu y desvío sigma. Los valores por defecto
4     %son mu = 0, sigma = 1 y N = 1. El vector x es vertical (de tamaño N x 1).
5     
6     if nargin < 1
7         mu = 0;    % media por defecto
8     end
9     if nargin < 2
10        sigma = 1; % desvío por defecto
11    end
12    if nargin < 3
13        N = 1;     % tamaño por defecto
14    end
15    
16    x = zeros(N,1); % inicializo
17    for n = 1:N
18        x(n) = randgauss(); % relleno el vector
19    end
20    x = mu + sigma .* x; % ajusto a mu y sigma
21    
22    end
\end{verbatim}
    

\subsection*{1.1.ii y 1.1.iii) $fdp$ gaussiana bidimensional [no] correlacionada;}

\begin{verbatim}
meds{1} = [50 100]; %
covs{1} = [2 0;     % definición de fdp no correlacionada
           0 1];    %
titulos{1} = ['fdp de una distribucion gaussiana bidimensional no ' ...
              'correlacionada'];

meds{2} = [50 100]; %
covs{2} = [2 1;     % definición de fdp correlacionada
           1 1];    %
titulos{2} = ['fdp de una distribucion gaussiana bidimensional ' ...
              'correlacionada'];

% Realizo la misma comparacion para los casos ii) y iii)
for k = 1:2
media = meds{k};
covar = covs{k};
titulo = titulos{k};
figure('units','normalized','outerposition',[0 0 1 1]);
suptitle({titulo; sprintf(['mu = [%0.0f %0.0f], '...
                           'sigma = [%0.0f %0.0f; %0.0f %0.0f]'],...
                          media, covar)})

N = 10000;                         % cantidad de muestras
x = randgaussXD(media, covar, N);  % conjunto de números aleatorios

[alturas, centros] = hist3(x,[15 15]);          % histograma sin normalizar
ancho1 = centros{1}(2) - centros{1}(1);         % ancho de las barras en x1
ancho2 = centros{2}(2) - centros{2}(1);         % ancho de las barras en x2
volumen = sum(ancho1 * sum(ancho2 * alturas));  % volumen de las barras
fdpexp = alturas ./ volumen;                    % fdp experimental

subplot(1,2,1);                      % grafica de la malla y teorica en 3D
mesh(centros{1},centros{2},fdpexp'); % mesh necesita la matriz traspuesta

% Contour plot de la fdp experimental
% hold on; contour3(centros{1},centros{2},0.01+fdpexp','k-'); hold off

% comparacion con la fdp teorica
NN = 100;
t1 = linspace(44,56,NN); t2 = linspace(96,104,NN);
fdpteor = zeros(NN);
for i = 1:NN
    for j = 1:NN
        fdpteor(i,j) = mvnpdf([t1(i) t2(j)],media,covar);
    end
end
hold on; contour3(t1,t2,0.005+fdpteor','k-'); hold off
axis tight;
daspect([max(daspect)*[1 1] 2]);% relacion de aspecto en x1 y x2

ylabel('x_2'); xlabel('x_1'); zlabel('p(x_1,x_2)');
legend('fdp experimental','fdp teorica', 'Location','West')
title('Superficie de nivel')

% curvas de nivel de la teorica y la experimental
subplot(1,2,2);
% contour necesita la matriz traspuesta
contour(centros{1},centros{2},fdpexp',5,'k');          % fdp experimental
hold on; contour(t1,t2,0.01+fdpteor',5,'b:'); hold off % fdp teorica
axis equal; axis([46 54 97 103]);

ylabel('x_2'); xlabel('x_1');
title('Curvas de nivel')
legend('fdp experimental','fdp teorica')

end
\end{verbatim}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Ejercicio1_02.png}
	\caption{Comparación fdp-2D teórica y experimental.}
	\label{ejercicio12}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Ejercicio1_03.png}
	\caption{Comparación fdp-2D teórica y experimental.}
	\label{ejercicio13}
\end{figure}

Para normalizar el histograma se dividió por la suma total de los volúmenes de todos los prismas, análogo al caso unidimensional donde se sumaron las áreas de las barras.


Para la comparación gráfica con la distribución teórica se utilizaron superficies y curvas de nivel. En ambos casos se comprobó el parecido entre la $pdf$ teórica y experimental.


Debajo se muestra el código de las dos funciones utilizadas para generar los conjuntos de números de una distribución normal bidimensional. \texttt{mvrandgauss.m} utiliza \texttt{randgauss.m} como punto de partida e incluye la demostración del funcionamiento dentro de su código.



\subsubsection*{mvrandgauss.m}

\begin{verbatim}
1     function x = mvrandgauss(MU, SIGMA)
2     %MVRANDGAUSS genera un número aleatorio de dimensión D proveniente de una 
3     % distribución gaussiana multidimensional cuyas medias quedan definidas
4     % por el vector MU (horizontal, de tamaño D x 1) y la matriz de covarianza 
5     % SIGMA (cuadrada, de tamaño D x D)
6     % Los valores por defecto son MU = 0 y SIGMA = matriz identidad.
7     
8     if nargin < 1
9         MU = 0;
10    end
11    if nargin < 2
12        SIGMA = eye(length(MU));
13    end
14    D = size(SIGMA,1);
15    
16    if (size(MU,2) ~= D) || (size(MU,1) ~= 1) || (D ~= size(SIGMA,2))
17       error(message('Error en los tamaños de MU o SIGMA'));
18    end
19    
20    % Como SIGMA es definida positiva puede usarse la descomposición de Cholesky 
21    triangSup = chol(SIGMA); % SIGMA = triangSup' * triangSup
22    
23    % z es un número aleatorio normal estándar de tamaño 1 x D
24    z = randgauss1D(0,1,D)';  % E[z' * z] = I
25    
26    x = z * triangSup + MU;
27    
28    % Demostración de que la expresión anterior devuelve el resultado esperado
29    % -------------------------------------------------------------------------
30    % Si se toma x = z * triangSup se cumple que:
31    % E[x' * x] = E[(z * triangSup)' * (z * triangSup)]
32    %           = E[ triangSup' * z' * z * triangSup ]
33    %           = triangSup' * E[ z' * z ] * triangSup
34    %           = triangSup' * I * triangSup
35    %           = triangSup' * triangSup
36    %           = SIGMA
37    % Entonces z * triangSup cumple con la matriz de covarianza dada SIGMA
38    % Por último desplazo la distribución con MU sin alterar  E[x' * x]
39    
40    end
\end{verbatim}
    

\subsubsection*{randgaussXD.m}

\begin{verbatim}
1     function x = randgaussXD(MU, SIGMA, N)
2     %RANDGAUSSXD(MU, SIGMA, N) genera un conjunto de números aleatorios de 
3     % dimensión D provenientes de una distribución gaussiana multidimensional, 
4     % cuyas medias quedan definidas por el vector MU (horizontal, de tamaño 
5     % 1 x D), la matriz de covarianza SIGMA (cuadrada, de tamaño D x D) y la
6     % cantidad de numeros N (1 por defecto). 
7     %
8     % x es de tamaño N x D, donde cada número de dimensión D se ubica por
9     % renglón.
10    
11    if nargin < 3
12        N = 1;     % tamaño por defecto
13    end
14    
15    x = zeros(N,length(MU)); % inicializo
16    for n = 1:N
17        x(n,:) = mvrandgauss(MU, SIGMA); % Conjunto de números aleatorios
18    end
19    
20    end
\end{verbatim}
    

\subsection{Ejercicio 1.2}


Compruebe numéricamente el teorema del límite central mediante la suma de números aleatorios con distribución uniforme.

\begin{verbatim}
repeticiones = 10000; % veces a repetir el calculo del promedio
N = [1 2 4 40];       % numeros a promedior
promedio = zeros(1,repeticiones);
figure
for n = 1:length(N)
    for r = 1:repeticiones
        % suma de N números aleatorios en [-1, +1]
        promedio(r) = (1/N(n)) * sum(2*rand(1,N(n))-1);
    end
    subplot(2,2,n)
    hist(promedio,40)
    title(['Promedio entre ' num2str(N(n)) ' numeros'])
end
\end{verbatim}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Ejercicio1_04.png}
	\caption{Convergencia a una distribución gaussiana del promedio de números aleatorios.}
	\label{ejercicio14}
\end{figure}

A medida que más números son utilizados para calcular el promedio la distribución se asemeja más y más a una gaussiana.


\subsection{Ejercicio 1.3}


Estime numéricamente y compare con el valor teorico, la entropia de una variable aleatoria con distribucion: i) laplaciana; ii) gaussiana y iii) uniforme.

\begin{verbatim}
N=1000;

% mantengo media y desvio para las 3 distribuciones
media = 3;
desvio = 2;

% numeros provenientes de una distribución gaussiana
xgauss = media + desvio*randn(N,1);

% numeros provenientes de una distribución uniforme
ini = media - sqrt(3) * desvio;
fin = media + sqrt(3) * desvio;
xunif = ini + (fin - ini) .* rand(N,1);

% numeros provenientes de una distribución laplaciana
media = 5;
beta  = desvio/sqrt(2);
xlap  = randlap(N,media,beta);

% Calculo de entropias teoricas y empíricas
HgaussTeorica  = 0.5 * (1 + log(2*pi*desvio.^2) )
HgaussEmpirica = entropia(xgauss)

HunifTeorica  = log(fin - ini)
HunifEmpirica = entropia(xunif)

HlapTeorica  = 1 + log(2*beta)
HlapEmpirica = entropia(xlap)
\end{verbatim}

Los valores obtenidos son:
\begin{verbatim}
HgaussTeorica  =   2.1121

HgaussEmpirica =   2.9484

HunifTeorica   =   1.9356

HunifEmpirica  =   3.3845

HlapTeorica    =   2.0397

HlapEmpirica   =   2.3636
\end{verbatim}
    
La entropía teórica de cada distribución esta calculada para la versión continua de la función de densidad de probabilidad ($fdp$). En cambio, la entropía empirica es calculada por media de una aproximación de la $fdp$. Esto ocasiona las diferencias observadas en los valores anteriores. Además es sabido que la entropía discreta no es equivalente a la entropía teórica




















\clearpage



\section{Ejercicio 2 - Clasificación estadística de patrones}

\subsection{Ejercicio 2.1}

Sea un clasificador geométrico lineal definido por:
\begin{itemize}
\item $g_1(\mathbf{x}) = - x_1$ 
\item $g_2(\mathbf{x}) = x_1 + x_2 - 1$
\item $g_3(\mathbf{x}) = x_1 - x_2 - 1$
\end{itemize}

\begin{description}
\item[a)] Calcule y grafique las fronteras y regiones de decisión.
\end{description}

Para calcular las fronteras de decisión desarrollo las siguientes inecuaciones:
\begin{align*}
g_1(\mathbf{x}) &> g_2(\mathbf{x})   &  g_1(\mathbf{x}) &> g_3(\mathbf{x})  &  g_2(\mathbf{x}) &> g_3(\mathbf{x}) \\
- x_1           &> x_1 + x_2 - 1     &  - x_1           &> x_1 - x_2 - 1    &  x_1 + x_2 - 1   &> x_1 - x_2 - 1   \\
  x_2           &< - 2 x_1 + 1       &  x_2             &> 2 x_1 + 1        &  x_2             &> 0
\end{align*}

Para graficar las regiones de decisión (Figura \ref{ejercicio21}) considero que:
\begin{itemize}
\item $\mathbf{x} \in C_1 \;\; \mathrm{si} \;\; [g_1(\mathbf{x}) > g_2(\mathbf{x})] \wedge  [g_1(\mathbf{x}) > g_3(\mathbf{x})]$

\item $\mathbf{x} \in C_2 \;\; \mathrm{si} \;\; [g_2(\mathbf{x}) > g_1(\mathbf{x})] \wedge  [g_2(\mathbf{x}) > g_3(\mathbf{x})]$

\item $\mathbf{x} \in C_3 \;\; \mathrm{si} \;\; [g_3(\mathbf{x}) > g_1(\mathbf{x})] \wedge  [g_3(\mathbf{x}) > g_2(\mathbf{x})]$
\end{itemize}

\begin{figure}[hb]
	\centering
	\includegraphics[width=.65\textwidth]{ejercicio21}
	\caption{Regiones y fronteras del clasificador geométrico lineal.}
	\label{ejercicio21}
\end{figure}

\begin{description}
\item[b)] Clasifique los puntos (2,1); (2,-1); (-1,1); (-1,-1).
\begin{itemize}
\item $\mathrm{p_1}:\; (2,1) \in C_2$
\item $\mathrm{p_2}:\; (2,-1) \in C_3$
\item $\mathrm{p_3}:\; (-1,1) \in C_1$
\item $\mathrm{p_4}:\; (-1,-1) \in C_1$
\end{itemize}
\end{description}



\subsection{Ejercicio 2.2}

Sean $A$ y $B$ dos clases de igual probabilidad a priori definidas por:
\begin{itemize}
\item $P(\mathbf{x}|A) \rightsquigarrow \mathcal{N}\left( 
\left(\begin{matrix}3\\0 \end{matrix}\right) ,
\left(\begin{matrix}1 & 0\\0 & 1 \end{matrix} \right) 
\right)$ 
\item $P(\mathbf{x}|B) \rightsquigarrow \mathcal{N}\left( 
\left(\begin{matrix}0\\3 \end{matrix}\right) ,
\left(\begin{matrix}1 & 0\\0 & 1 \end{matrix} \right) 
\right)$ 
\end{itemize}

\begin{description}
\item[a)] Construya un clasificador gaussiano y encuentre la frontera.
\end{description}
Dado que se pretende construir un clasificador gaussiano las funciones discriminantes son las probabilidades a posteriori:
\begin{itemize}
\item $g_A(\mathbf{x})=P(A|\mathbf{x}) = P(\mathbf{x}|A) P(A) / P(\mathbf{x})$ y
\item $g_B(\mathbf{x})=P(B|\mathbf{x}) = P(\mathbf{x}|B) P(B) / P(\mathbf{x})$.
\end{itemize} 
Como $(P(A)/P(\mathbf{x}))=(P(B)/P(\mathbf{x}))$ es un factor constante que afecta por igual a ambas funciones discriminantes se pueden simplificar a:
\begin{itemize}
\item $g_A(\mathbf{x})= P(\mathbf{x}|A) = \mathcal{N}\left( {\boldsymbol \mu}_A,{\boldsymbol\Sigma}_A \right) = \mathcal{N}\left( 
\left(\begin{matrix}3\\0 \end{matrix}\right) ,
\left(\begin{matrix}1 & 0\\0 & 1 \end{matrix} \right) 
\right)$ y
\item $g_B(\mathbf{x})= P(\mathbf{x}|B) = \mathcal{N}\left( {\boldsymbol \mu}_B,{\boldsymbol\Sigma}_B \right) = \mathcal{N}\left( 
\left(\begin{matrix}0\\3 \end{matrix}\right) ,
\left(\begin{matrix}1 & 0\\0 & 1 \end{matrix} \right) 
\right)$.
\end{itemize}
La expresión de la normal puede simplificarse aún más en este caso, sin pérdida de la capacidad discriminante de las funciones. En primer lugar se utiliza el hecho de que las matrices de covarianzas son la matriz identidad. 
\[\mathcal{N}\left( {\boldsymbol \mu},{\boldsymbol\Sigma} \right)\]
\[\mathcal{N}\left( {\boldsymbol \mu},{\boldsymbol I} \right)\]
\[\frac{1}{\sqrt{(2\pi)^2|\boldsymbol I|}}
\exp\left(-\frac{1}{2}({\mathbf x}-{\boldsymbol\mu})^T{\boldsymbol I}^{-1}({\mathbf x}-{\boldsymbol\mu})
\right)\]
\[\frac{1}{\sqrt{(2\pi)^2}}
\exp\left(-\frac{1}{2}({\mathbf x}-{\boldsymbol\mu})^T({\mathbf x}-{\boldsymbol\mu})
\right)\]
En segundo lugar, el factor que afecta a la exponencial es el mismo en ambas funciones y puede eliminarse. 
\[\exp\left(-\frac{1}{2}({\mathbf x}-{\boldsymbol\mu})^T({\mathbf x}-{\boldsymbol\mu})\right)\]
\[\exp\left(-\frac{1}{2}\parallel{\mathbf x}-{\boldsymbol\mu}\parallel^2
\right)\]
En último lugar se aplica el logaritmo natural, función que no altera las regiones de decisión, y se quita el factor constante 1/2.
\[-\frac{1}{2}\parallel{\mathbf x}-{\boldsymbol\mu}\parallel^2\]
\[-\parallel{\mathbf x}-{\boldsymbol\mu}\parallel^2 \]
Entonces las funciones discriminantes resultan ser:
\begin{itemize}
\item $g_A(\mathbf{x})= -\parallel{\mathbf x}-{\boldsymbol\mu}_A\parallel^2$ y
\item $g_B(\mathbf{x})= -\parallel{\mathbf x}-{\boldsymbol\mu}_B\parallel^2$,
\end{itemize}
donde el máximo entre ambas funciones indica la pertenencia a $A$ o $B$.

Para encontrar la frontera de decisión igualo ambas funciones:
\begin{align*}
g_A(\mathbf{x}) &= g_B(\mathbf{x}) \\
-\parallel{\mathbf x}-{\boldsymbol\mu}_A\parallel^2 &= -\parallel{\mathbf x}-{\boldsymbol\mu}_B\parallel^2 \\
\parallel{\mathbf x}-{\boldsymbol\mu}_A\parallel^2 &= \parallel{\mathbf x}-{\boldsymbol\mu}_B\parallel^2 \\
(x_1 - 3)^2 + x_2^2 &= x_1^2 +(x_2 - 3)^2\\
x_1^2 - 6 x_1 + 9 + x_2^2 &= x_1^2 + x_2^2 - 6 x_2 + 9\\
x_1 &= x_2 
\end{align*}

\begin{description}
\item[b)] Realice una representación gráfica del problema.
\end{description}
En la Figura \ref{ejercicio22} se muestra claramente la frontera de decisión (la recta) y la etiqueta de cada región. Las curvas representan las curvas de nivel de la función discriminante ganadora en cada región, a medida que se alejan de la recta van creciendo en valor.  

\begin{figure}[h]
	\centering
	\includegraphics[scale= .5]{ejercicio22}
	\caption{Regiones y fronteras del clasificador gaussiano.}
	\label{ejercicio22}
\end{figure}


\clearpage


\subsection{Ejercicio 2.3}

Implemente un clasificador gaussiano por ML para el mini-corpus de muestras proporcionado, realizando una representación gráfica de la situación:

\begin{description}
\item[a)] Clasifique los datos del conjunto de entrenamiento y calcule la tasa de aciertos.

\item[b)] A partir de los parámetros obtenidos genere nuevos datos de prueba, clasifíquelos y compare con el resultado anterior.
\end{description}

\begin{verbatim}
% Cargo los datos (patrones con sus clases respectivas)
data = load('gaussDATA.txt', '-ascii'); % cargo los datos
x = data(:,1:2); % patrones
c = data(:,3);   % identificador de clase
N = length(c);   % cantidad de ejemplos

c1 = find(c== 1); % indices de clase 1
c2 = find(c== 2); % indices de clase 2
c3 = find(c== 3); % indices de clase 3
c4 = find(c== 4); % indices de clase 4

hold on
plot(x(c1,1), x(c1,2), 'ok')
plot(x(c2,1), x(c2,2), 'xk')
plot(x(c3,1), x(c3,2), 'dk')
plot(x(c4,1), x(c4,2), 'sk')
hold off

xlabel('x_1'); ylabel('x_2');
legend('clase 1','clase 2','clase 3','clase 4')
title('Distribución de los patrones separados por clase')
\end{verbatim}

\begin{figure}
\includegraphics [width=0.9\textwidth]{Ejercicio2_01.png}
\caption{Distribución de los patrones del mini-corpus indicando la clase de cada uno.}
\label{fig:ejercicio231}
\end{figure}


Por la gráfica de la Figura \ref{fig:ejercicio231} se deduce que deberán utilizarse gaussianas con media desconocida y covarianza no isotrópica desconocida. Separo los patrones de entrenamiento para estimar las medias y matrices de covarianzas de cada clase.

\begin{verbatim}
% Divido los patrones a utilizar en el entrenamiento de los de testeo
nentre = floor(0.8 * N); % 80% de los patrones para entrenamiento
ntest = N - nentre;      % 20% de los patrones para testeo
ientre = randperm(N,nentre); % indices utilizados en el entrenamiento
itest = setdiff(1:N,ientre); % indices utilizados para el testeo

% Estimación de los parámetros para cada clase, con los patrones de
% entrenamiento
% medias estimadas
u1 = mean(x(intersect(c1,ientre),:));
u2 = mean(x(intersect(c2,ientre),:));
u3 = mean(x(intersect(c3,ientre),:));
u4 = mean(x(intersect(c4,ientre),:));
% matrices de covarianzas estimadas
sig1 = cov(x(intersect(c1,ientre),:));
sig2 = cov(x(intersect(c2,ientre),:));
sig3 = cov(x(intersect(c3,ientre),:));
sig4 = cov(x(intersect(c4,ientre),:));

% clasifico los patrones de entrenamiento
centre = zeros(size(c)); % clases asignadas durante el entrenamiento
for n = ientre
    xn = x(n,:); % voy leyendo de a un dato

    % Calculo los valores de las funciones discriminantes para xn
    g1 = mvnpdf(xn, u1, sig1);
    g2 = mvnpdf(xn, u2, sig2);
    g3 = mvnpdf(xn, u3, sig3);
    g4 = mvnpdf(xn, u4, sig4);

    % Busco cual es la clase ganadora, asigno 0 si cae en la frontera
    if     g1 > max([g2 g3 g4])
        centre(n) = 1;
    elseif g2 > max([g1 g3 g4])
        centre(n) = 2;
    elseif g3 > max([g1 g2 g4])
        centre(n) = 3;
    elseif g4 > max([g1 g2 g3])
        centre(n) = 4;
    else
        centre(n) = 0;
    end
end

% busco cuales patrones fueron confundidos al clasificar
cmal = intersect(find(c~=centre), ientre); % patrones mal identificados
cbien = setdiff(ientre,cmal); % patrones identificados correctamente

fprintf('La tasa de aciertos en el entrenamiento es de %0.2f %%\n', ...
        100*length(cbien)/nentre);

hold on; plot(x(cmal,1), x(cmal,2), '.r'); hold off
\end{verbatim}

%\begin{figure}[h]
%\includegraphics [width=0.9\textwidth]{Ejercicio2_02.png}
%\caption{Los patrones indicados con puntos rojos fueron clasificados incorrectamente durante el entrenamiento.}
%\label{fig:ejercicio232}
%\end{figure}

En el entrenamiento se utilizaron el 80\% de los patrones de la Figura \ref{fig:ejercicio233}. Aquellos indicados con puntos rojos fueron clasificados incorrectamente durante el entrenamiento. Considerando sólo los patrones de entrenamiento se obtuvo el siguiente reporte:

\begin{verbatim}La tasa de aciertos en el entrenamiento es de 87.27 %
\end{verbatim}

Ahora se clasifican los patrones para testeo y se calcula la tasa de reconocimiento.

\begin{verbatim}
% clasifico los patrones de testeo
% --------------------------------
ctest = zeros(size(c)); % clases asignadas durante el testeo
for n = itest
    xn = x(n,:); % voy leyendo de a un dato

    % Calculo los valores de las funciones discriminantes para xn
    g1 = mvnpdf(xn, u1, sig1);
    g2 = mvnpdf(xn, u2, sig2);
    g3 = mvnpdf(xn, u3, sig3);
    g4 = mvnpdf(xn, u4, sig4);

    % Busco cual es la clase ganadora, asigno 0 si cae en la frontera
    if     g1 > max([g2 g3 g4])
        ctest(n) = 1;
    elseif g2 > max([g1 g3 g4])
        ctest(n) = 2;
    elseif g3 > max([g1 g2 g4])
        ctest(n) = 3;
    elseif g4 > max([g1 g2 g3])
        ctest(n) = 4;
    else
        ctest(n) = 0;
    end
end

% busco cuales patrones fueron confundidos al clasificar
cmal = intersect(find(c~=ctest), itest); % patrones mal identificados
cbien = setdiff(itest,cmal); % patrones identificados correctamente

fprintf('La tasa de aciertos en el testeo es de %0.2f %%\n', ...
        100*length(cbien)/ntest);

hold on; plot(x(cmal,1), x(cmal,2), '*b'); hold off
\end{verbatim}

\begin{figure}[h]
\includegraphics [width=0.9\textwidth]{Ejercicio2_03.png}
\caption{Los patrones indicados con puntos rojos fueron clasificados incorrectamente durante el entrenamiento, y los indicados con asteriscos azules durante el testeo.}
\label{fig:ejercicio233}
\end{figure}

En el testeo se utilizaron el 20\% restante de los patrones de la Figura \ref{fig:ejercicio233}. Los indicados con asteriscos azules fueron clasificados incorrectamente. Luego del testeo se obtuvo el siguiente reporte:

\begin{verbatim}La tasa de aciertos en el testeo es de 78.57 %
\end{verbatim}
    
Por último se realizó una gráfica (Figura \ref{fig:ejercicio234}) representativa de las regiones definidas durante el entrenamiento.

\begin{verbatim}
T = 100;
x1 = linspace(-2,10,T);
x2 = linspace(-4,8,T);
[X1,X2] = meshgrid(x1,x2);
for i = 1:T
    for j = 1:T
        xn = [x1(i) x2(j)];
        gc1(i,j) = mvnpdf(xn, u1, sig1);
        gc2(i,j) = mvnpdf(xn, u2, sig2);
        gc3(i,j) = mvnpdf(xn, u3, sig3);
        gc4(i,j) = mvnpdf(xn, u4, sig4);
    end
end

gc1234 = cat(3, gc1, gc2, gc3, gc4);

[maxg, maxc] = max(gc1234,[],3);

figure
grises = linspace(0.7,0.9,64);
colormap(repmat(grises',1,3))
surf(X1,X2,maxc'-4,'EdgeColor','none')
hold on; contour(X1,X2,maxg',5,'k'); hold off;
view(2); axis square
xlabel('x_1'); ylabel('x_2');
title('Regiones de decisión')

text([u1(1) u2(1) u3(1) u4(1)], [u1(2) u2(2) u3(2) u4(2)],...
    {'C_1','C_2','C_3','C_4'})
\end{verbatim}

\begin{figure}[h]
\includegraphics [width=0.9\textwidth]{Ejercicio2_04.png}
\caption{Regiones de decisión utilizadas para clasificar. Las curvas de nivel corresponden a las funciones discriminantes en cada región.}
\label{fig:ejercicio234}
\end{figure}


\clearpage







\section{Ejercicio 3 - Análisis estadístico de datos - PCA}

\subsection{Ejercicio 3.1}


Implemente el algoritmo de PCA.


Debajo se copia el código fuente de la función implementada. La descripción de la misma se encuentra dentro del código.

\subsubsection*{mipca.m}

\begin{verbatim}
1     function [PC, autoval] = mipca( X )
2     %MIPCA devuelve la matriz que permite proyectar los datos X sobre las
3     %direcciones obtenidas por medio del análisis de componentes principales.
4     % X es una matriz de D por N, donde cada observación se acomoda como
5     % columna. D es la dimensión de cada observación y N es el
6     % número total de observaciones.
7     %
8     % PC es la matriz de transformación de la base original a las direcciones
9     % principales. Las columnas son los versores de la nueva base.
10    %
11    % autoval es un vector con los autovalores asociados a PC, en orden 
12    % decreciente.
13    
14    % Matriz de covarianza (internamente realiza la resta de la media)
15    S = cov(X'); % traspongo para que tome bien las observaciones
16    
17    % Obtengo la matriz de autovectores y una matriz con los autovalores de S
18    [autovec, autoval] = eig(S);
19    
20    % Ordeno la matriz de transformación por orden descendente de autovalores
21    PC = fliplr(autovec);
22    
23    % Autovalores ordenados por módulo decreciente
24    autoval = flipdim(diag(autoval),1);
25    
26    end
\end{verbatim}
    

\subsection{Ejercicio 3.2}


Escriba un programa que le permita generar datos aleatorios $\mathbf(x)$ a partir del siguiente modelo generativo lineal: $\mathbf(x) = \mathbf(A) \mathbf(s)$, donde $\mathbf(s)$ es el vector de fuentes (aleatorio) y $\mathbf(A)$ es la matriz de mezcla.

Debajo se copia el código fuente de la función implementada. La descripción de la misma se encuentra dentro del código.

\subsubsection*{mezclar.m}
\begin{verbatim}
1     function X = mezclar( A, s1, s2 )
2     %MEZCLAR X = ( A, s1, s2 ) dada la matriz de mecla A y las fuentes s1 y s2 
3     % (vectores) se generan dos mezclas x1 y x2 (cada una en un renglón de X).
4     
5     if size(s1,1) ~= 1
6         s1 = s1'; % convierte s1 en un vector horizontal si no lo es
7     end
8     if size(s2,1) ~= 1
9         s2 = s2'; % convierte s2 en un vector horizontal si no lo es
10    end
11    
12    S = vertcat(s1,s2);
13    X = A * S;
14    end
\end{verbatim}
    

\subsection{Ejercicio 3.3}

A partir de datos de dos mezclas, obtenidos mediante dos fuentes y una matriz de mezcla aleatoria, utilice PCA para lo siguiente:

\begin{enumerate}
   \item[a)] Pruebe con fuentes con distribución gaussiana y laplaciana, para matrices de mezcla con columnas ortogonales y no ortogonales.
   \item[b)] Para cada caso de los anteriores y cada etapa (fuentes, mezclas, señales separadas) dibuje un gráfico de dispersión de las variables.
   \item[c)] Luego de la separación obtenga la matriz $\mathbf{W}$ correspondiente.
\end{enumerate}
\begin{verbatim}
N = 300;

% Fuentes gaussianas y laplacianas
s{1} = randgauss1D(1, 1, N)';
s{2} = randgauss1D(3, 3, N)';
s{3} = randlap(N, 2 ,2)';
s{4} = randlap(N, 5 ,0.9)';
s{5} = randgauss1D(1, 1, N)';
s{6} = randlap(N, 2 ,2)';

% Se corroboran los signos para impedir que ambas mezclas sean
% proporcionales
signos = ones(2);
while isequal(signos(1,:),signos(2,:)) || isequal(signos(1,:), -signos(2,:))
    signos = sign(rand(2)-0.5);
end

% Generacion aleatoria de las matrices de mezcla
A{1} = repmat(2*rand(1,2)-1,2,1) .* signos;% Mezcla con columnas ortogonales
A{2} = (2*rand(2)-1) .* signos;            % Mezcla no ortogonal
titort{1} = 'ortogonales';
titort{2} = 'no ortogonales';
suptit{1} = 'Fuentes gaussianas';
suptit{2} = 'Fuentes laplacianas';
suptit{3} = 'Fuente gaussiana y laplaciana';

for ss = 1:3
    figure
    for aa = 1:2
        X = mezclar(A{aa},s{2*ss-1},s{2*ss}); % Mezcla las señales
        x1m = mean(X(1,:)); % media de la primera dimension
        x2m = mean(X(2,:)); % media de la segunda dimension

        W = mipca(X); % Obtengo la Matriz de Proyección de X sobre las
                      % direcciones principales, versores por columnas

        Y = W' * X; % Proyecto los datos sobre las direcciones principales
        % y_1  = w_11 * x_1 + w_21 * x_2
        % y_2  = w_12 * x_2 + w_22 * x_2

        subplot(2,3,1+3*(aa-1))
        scatter(s{2*ss-1}, s{2*ss}); axis equal;
        title({'Fuentes';['columnas ' titort{aa}]} )
        xlabel('s_1'); ylabel('s_2');

        subplot(2,3,2+3*(aa-1))
        scatter(X(1,:), X(2,:)); axis equal;
        hold on;
        plot(x1m + [0 5*W(1,1)],x2m + [0 5*W(2,1)], 'k','LineWidth',2)
        plot(x1m + [0 5*W(1,2)],x2m + [0 5*W(2,2)], 'k','LineWidth',2)
        hold off
        title({'Mezclas';['columnas ' titort{aa}]} )
        xlabel('x_1'); ylabel('x_2');

        subplot(2,3,3+3*(aa-1))
        scatter(Y(1,:), Y(2,:)); axis equal;
        title({'Señales separadas';['columnas ' titort{aa}]} )
        xlabel('y_1'); ylabel('y_2');

    end
    suptitle(suptit{ss})
end
\end{verbatim}


En las Figuras \ref{fig:ejercicio331}, \ref{fig:ejercicio332} y \ref{fig:ejercicio333} se muestra cada una de las etapas requeridas. En las gráficas de las mezclas se dibujan las direcciones principales identificadas mediante PCA. La fila superior corresponde a matrices de mezclas con columnas ortogonales, la inferior a columnas no ortogonales.

Las matrices de mezcla y separación utilizadas se listan a continuación, para cada caso:
\begin{verbatim}Fuentes gaussianas, columnas ortogonales
A = [0.94 -0.74],	 Ainv = [0.53 0.53],	 W = [-0.71 -0.70]
    [0.94 0.74],	        [-0.67 0.67],	     [0.70 -0.71]

Fuentes gaussianas, columnas no ortogonales
A = [-0.05 0.54],	 Ainv = [0.15 -1.22],	 W = [-0.98 0.21]
    [-0.82 0.07],	        [1.86 -0.11],	     [-0.21 -0.98]

Fuentes laplacianas, columnas ortogonales
A = [0.94 -0.74],	 Ainv = [0.53 0.53],	 W = [0.70 -0.72]
    [0.94 0.74],	        [-0.67 0.67],	     [0.72 0.70]

Fuentes laplacianas, columnas no ortogonales
A = [-0.05 0.54],	 Ainv = [0.15 -1.22],	 W = [0.06 -1.00]
    [-0.82 0.07],	        [1.86 -0.11],	     [1.00 0.06]

Fuente gaussiana y laplaciana, columnas ortogonales
A = [0.94 -0.74],	 Ainv = [0.53 0.53],	 W = [-0.66 -0.75]
    [0.94 0.74],	        [-0.67 0.67],	     [0.75 -0.66]

Fuente gaussiana y laplaciana, columnas no ortogonales
A = [-0.05 0.54],	 Ainv = [0.15 -1.22],	 W = [-0.99 0.12]
    [-0.82 0.07],	        [1.86 -0.11],	     [-0.12 -0.99]
\end{verbatim}

\begin{figure}
\includegraphics [width=\textwidth]{Ejercicio3_01.png}
\caption{PCA aplicado a fuentes gaussianas.}
\label{fig:ejercicio331}
\end{figure}
\begin{figure}
\includegraphics [width=\textwidth]{Ejercicio3_02.png}
\caption{PCA aplicado a fuentes laplacianas.}
\label{fig:ejercicio332}
\end{figure}
\begin{figure}
\includegraphics [width=\textwidth]{Ejercicio3_03.png}
\caption{PCA aplicado a una fuente gaussiana y otra laplaciana.}
\label{fig:ejercicio333}
\end{figure}


\begin{enumerate}
   \item[d)] ¿La matriz de separación $\mathbf{W}$ es la inversa de la matriz de mezcla $\mathbf{A}$ utilizada?
\end{enumerate}

Para responder a la pregunta desarrollo la siguiente expresión:

$\mathbf{y} = \mathbf{W} \mathbf{x} = \mathbf{W} \mathbf{A} \mathbf{s}$

Si la matriz de separación $\mathbf{W}$ fuera la inversa de la matriz de mezcla $\mathbf{A}$ entonces utilizando PCA se podría recuperar las fuentes. Sin embargo, en las pruebas anteriores no se corrobora que $\mathbf{W}$ sea la inversa de $\mathbf{A}$. Al proyectar sobre las direcciones principales se provoca una rotación de la distribución de mezclas y esto no asegura que se recuperen las fuentes. El único caso en el que podría ser posible recuperar las fuentes es si la mezcla misma es una rotación de la distribución de las fuentes, en cuyo caso, si las direcciones principales coinciden con las direcciones de las fuentes se podría llegar a recuperar las mismas.

La matriz de covarianza de las señales proyectadas no se ven afectadas por las medias de las mezclas. Sin embargo, las medias de las señales proyectadas si cambian de acuerdo al valor de las medias de las mezclas. Esto no afecta la forma de la distribución sólo la desplaza a otro punto del espacio. Quizá sería una buena practica quitarle la media a las señales recuperadas.

\begin{enumerate}
   \item[e)] ¿Cómo se afecta este resultado si agrega una componente de ruido gaussiano al modelo generativo?
\end{enumerate}

La situación antes descripta no cambia porque el modelo contenga o no ruido. PCA no necesita realizar ninguna hipótesis respecto al modelo que generó los datos, sólo permite observar los datos dados desde otra perspectiva.



\clearpage




\section{Ejercicio 4 - Análisis estadístico de datos - ICA}

\subsection{Ejercicio 4.1}


Implemente el algoritmo de FastICA con aprendizaje deflacionario.


Debajo se copia el código fuente de la función implementada. La descripción de la misma se encuentra dentro del código.


\subsubsection*{fastica.m}

\begin{verbatim}
1     function [sepMat] = fastica( X )
2     %[sepMat] = fastica( X ) devuelve la matriz de separación sepMat
3     %   Los versores de la descomposición se ubican como columnas en la matriz
4     %   de separación.
5     %   Las mezclas X deben ser de tamaño N x M, donde N corresponde a la
6     %   cantidad de mezclas y M a la cantidad de muestras que tiene cada señal.
7     %   Las mezclas en X deben tener media cero y la matriz de covarianza debe
8     %   ser identidad (realizar whitening de ser necesario)
9     
10    tolerancia = 1e-6; 
11    optimizando = 0;  %
12    [N, M] = size(X); % N-dimensiones, M muestras
13    
14    for p = 1:N
15        error = 1;    % inicia el ciclo de optimización
16        colineal = 0; % inicia el ciclo de optimización
17        w(:,p) = 2*rand(N,1)-1; % inicialización aleatoria de wp
18        
19        while (error > tolerancia) && ~colineal
20            want = w(:,p);
21            
22            % G(y) = (1/a) * log cosh (ay)
23            % g(y) = tanh(ay)
24            % gprima(y) = a(1-tanh(ay)^2)
25            a = 1;
26            g = ones(N,1) * tanh(a * want' * X) ;% g es de tamaño N x M
27                                                 % ones genera N filas iguales
28                                                 % de g evaluada en cada
29                                                 % a*want'*x
30            gprima = a * (1 - tanh(a * want' * X).^2); % gprima es de tamaño 1 x M
31            
32            % Expresion para cada muestra
33            % wnuevo = E[g(w'*x)*x] - E[gprima(w'*x)]*w
34            % Expresion usando el vector X de todas las muestras
35            w(:,p) = (-1) *(mean(g .* X, 2) - mean(gprima) * want);
36                 
37            % El valor esperado de la ecuación original es estimado por el 
38            % valor medio
39            % g .* X es el equivalente de g(w'*x)*x considerando todos los
40            % ejemplos
41            
42            % se ortogonaliza respecto a las wp ya encontradas
43            for j = 1:p-1
44                w(:,p) = w(:,p) - (w(:,p)' * w(:,j)) * w(:,j);
45            end
46            
47            % se normaliza wp
48            w(:,p) = w(:,p) ./ norm(w(:,p));
49            
50            error = norm(w(:,p) - want); % chequea convergencia de w(:,p)
51            colineal = (1-abs(dot(want,w(:,p)))) < tolerancia; % si es colineal
52                                                               % finalizo la
53                                                               % búsqueda
54        end
55    end
56    
57    sepMat = w; % encontrados todos los wp los paso a la salida de la función
58    
59    end
\end{verbatim}
    

\subsection{Ejercicio 4.2}

A partir de datos de dos mezclas, obtenidos mediante dos fuentes laplacianas y una matriz de mezcla aleatoria, utilice FastICA para lo siguiente:

\begin{enumerate}
   \item[a)] Para cada etapa (fuentes, mezclas, señales blanqueadas, señales   separadas) dibuje un gráfico de dispersión de las variables.
   \item[b)] Luego de la separación, estime las matrices $\mathbf{P}$ y $\mathbf{D}$.
\end{enumerate}
\begin{verbatim}
N = 1000; % Cantidad de muestras

% Fuentes laplacianas
s{1} = randlap(N, 2 , 2)';
s{2} = randlap(N, -5,0.4)';

% Se corroboran los signos para impedir que las mezclas sean iguales
signos = ones(2);
while isequal(signos(1,:),signos(2,:)) || isequal(signos(1,:), -signos(2,:))
    signos = sign(rand(2)-0.5);
end
% Generacacion de la matriz de mezcla
A = (2*rand(2)-1) .* signos;  % Mezcla no ortogonal
% A = [0.6 0.8; 0.5 -0.2];

% suptit{1} = 'Fuentes laplacianas';


%Mezclas de las dos fuentes
% -------------------------

X = mezclar(A,s{1},s{2});


%Blanqueo de las mezclas mediante PCA
% -----------------------------------

[E, lambdas] = mipca(X);
% E: matriz cuyas columnas son los versores de las direcciones
%    principales.
% lambdas: es un vector con los autovalores

Dmenos1medio = diag(sqrt(lambdas.^(-1)));% matriz con las inversas de los
                                         % lambdas en la diagonal principal

Xmean = repmat(mean(X,2), 1, size(X,2)); % media de las mezclas

Z = Dmenos1medio * E'  * (X - Xmean);    % señales blanqueadas


%Separación con FastICA
% ----------------------

W = fastica(Z); % me devuelve la matriz de separación

Y = W' * Z; % Separación de las fuentes mediante la matriz dada por fastica
\end{verbatim}


\subsection*{Estimando P y D}


Como se cumple que $\mathbf{y} = \mathbf{W}^T\mathbf{x} = \mathbf{W}^T\mathbf{A}\mathbf{s}$, es posible fijarse cual de las dos fuentes aporta mas a cada señal recuperada y así poder determinar si están permutadas las señales recuperadas con las fuentes.


Así, en primer lugar busco la matriz $\mathbf{P}$ que cumpla $\mathbf{P} \mathbf{D} = \mathbf{W}^T\mathbf{A}$. Hay solo dos posibilidades para esta matriz, la identidad, o un matriz con sólo unos en la diagonal secundaria. Para esto me fijo en cada renglón de la matriz $\mathbf{W}^T\mathbf{A}$ y defino una matriz $\mathbf{P}$ a partir de los máximos componentes por renglón.

\begin{verbatim}
WA = W' * A;
[~, maxwa] = max(abs(WA),[],2);

P = zeros(2);
P(1,maxwa(1)) = 1;
P(2,maxwa(2)) = 1;
\end{verbatim}

La anterior forma de encontrar $\mathbf{P}$ no está funcionando como se esperaba por lo que para decidir la permutación realizo una comparación entre las señales recuperadas y las fuentes.

\begin{verbatim}
s1y1 = dot(s{1}-mean(s{1},2),Y(1,:)) ./ size(Y,2); % fuente 1 vs recup 1
s1y2 = dot(s{1}-mean(s{2},2),Y(2,:)) ./ size(Y,2); % fuente 1 vs recup 2
s2y1 = dot(s{2}-mean(s{1},2),Y(1,:)) ./ size(Y,2); % fuente 2 vs recup 1
s2y2 = dot(s{2}-mean(s{2},2),Y(2,:)) ./ size(Y,2); % fuente 2 vs recup 2

auxs1 = [s1y1, s1y2]; % el maximo me dice cual corresponde a fuente 1
auxs2 = [s2y1, s2y2]; % el maximo me dice cual corresponde a fuente 2
[~, s1max] = max(abs(auxs1));
[~, s2max] = max(abs(auxs2));

P = zeros(2) ;
P(1,s1max) = 1; % asigno los 1 de acuerdo a lo encontrado
P(2,s2max) = 1; % asigno los 1 de acuerdo a lo encontrado

% como P y su inversa son iguales calculo D = Pinv * W' * A

D = P * WA;

fprintf(['P = [%0.2f %0.2f],\t D = [%0.2f %0.2f]\n'...
         '    [%0.2f %0.2f],\t     [%0.2f %0.2f]\n\n'],...
          P(1,:), WA(1,:), P(2,:), WA(2,:) );


% Graficando cada etapa
subplot(2,2,1)
scatter(s{1}, s{2}); axis equal;
title({'Fuentes'})
xlabel('s_1'); ylabel('s_2');

subplot(2,2,2)
scatter(X(1,:), X(2,:)); axis equal;
title({'Mezclas'})
xlabel('x_1'); ylabel('x_2');

subplot(2,2,3)
scatter(Z(1,:), Z(2,:)); axis equal;
title({'Señales blanqueadas'})
xlabel('z_1'); ylabel('z_2');

subplot(2,2,4)
scatter(Y(1,:), Y(2,:)); axis equal;
title({'Señales separadas'})
xlabel('y_1'); ylabel('y_2');
\end{verbatim}


\begin{figure}
\includegraphics [width=\textwidth]{Ejercicio4_01.png}
\caption{Señales en cada etapa al aplicar FastICA.}
\label{fig:ejercicio41}
\end{figure}

En la Figura \ref{fig:ejercicio41} se muestra cada una de las etapas requeridas. Las matrices solicitadas se escriben a continuación.

\begin{verbatim}P = [0.00 1.00],	 D = [0.85 -0.14]
    [1.00 0.00],	     [-0.44 -0.35]
\end{verbatim}

\begin{verbatim}
% Comparación entre señales separadas y fuentes
figure
subplot(2,2,1)
plot(s{1}(1:60)); ylim([-10, 10]);
ylabel('s_1')

subplot(2,2,2)
plot(s{2}(1:60)); ylim([-10, 10]);
ylabel('s_2');

subplot(2,2,3)
plot(Y(1,1:60)); ylim([-10, 10]);
ylabel('y_1');

subplot(2,2,4)
plot(Y(2,1:60)); ylim([-10, 10]);
ylabel('y_2');
\end{verbatim}

\begin{figure}
\includegraphics [width=\textwidth]{Ejercicio4_02.png}
\caption{Segmento de las fuentes (arriba) y las señales separadas(abajo). La comparación permite comprobar la matriz $\mathbf{P}$ y $\mathbf{D}$.}
\label{fig:ejercicio41}
\end{figure}


\begin{enumerate}
   \item[c)] ¿La matriz de separación $\mathbf{W}$ es la inversa de la matriz de mezcla $\mathbf{A}$ utilizada?
\end{enumerate}

No lo es, en el caso ideal lo sería. En la práctica se espera obtener $\mathbf{P} \mathbf{D} = \mathbf{W}^T\mathbf{A}$, donde $\mathbf{P}$ es una matriz de permutación y \ensuremath{\backslash}mathbf\{D\} es una matriz de escalado (también involucra el signo para invertir la señal de ser necesario).

\begin{enumerate}

   \item[d)]  ¿Cómo se afecta este resultado si agrega una componente de ruido gaussiano al modelo generativo?
\end{enumerate}

La base del funcionamiento de ICA está en maximizar la lejanía de las distribuciones de cada señal recuperada de la distribución gaussiana. Si el ruido gaussiano tiene una potencia importante en relación a las fuentes es de esperar que no se logren recuperar. Si el ruido no enmascara por demás las fuentes es posible recuperar las fuentes.

\begin{enumerate}

   \item[e)]  ¿Qué ocurre si una de las fuentes es gaussiana? ¿Y si ambas lo son?
\end{enumerate}

Si una de las fuentes es gaussiana todavía es posible encontrar las fuentes, si las dos lo son no es posible.

\clearpage





\section{Ejercicio 5 - Aprendizaje basado en redes y algoritmos clásicos}

\subsection{Ejercicio 5.1}

Genere un conjunto de datos aleatorios bidimensionales separados en dos clases gaussianas correlacionadas con cierto grado de solapamiento.

\begin{verbatim}
N = 200; % cantidad de muestras

mu1 = [1 2];                % media clase 1 (+)
sigma1 = [0.9 0.3; 0.3 1.7];  % covarianza clase 1 (+)
mu2 = [4 4];                  % media clase 2 (o)
sigma2 = [1.5 -0.7; -0.7  3]; % covarianza clase 2 (o)

x1 = mvnrnd(mu1, sigma1, N); % ejemplos de la clase 1 (+)
x2 = mvnrnd(mu2, sigma2, N); % ejemplos de la clase 2 (o)
eti1 = ones(N,1);            % etiquetas de los ejemplos de la clase 1 (+)
eti2 = (-1)*ones(N,1);       % etiquetas de los ejemplos de la clase 2 (o)

% Graficacion de las distribuciones
hold on
plot(x1(:,1), x1(:,2),'+b')
plot(x2(:,1), x2(:,2),'or')
legend('clase 1','clase 2')
title('Distribuciones de los patrones')
xlabel('x_1'); ylabel('x_2');
axis equal
hold off

% Separacion entre entrenamiento y testeo, a utilizar cuando corresponda
nentre = floor(0.7 * N); % 70% de los patrones para entrenamiento
ntest = N - nentre;      % 30% de los patronespara testeo

% ejemplos de entrenamiento
xentre = vertcat(x1(1:nentre,:), x2(1:nentre,:));
% etiquetas para entrenamiento supervisado
etientre = vertcat(eti1(1:nentre), eti2(1:nentre));
% ejemplos de testeo
xtest = vertcat(x1(nentre+1:end,:), x2(nentre+1:end,:));
% etiquetas para testeo
etitest = vertcat(eti1(nentre+1:end), eti2(nentre+1:end));

% Ordeno de forma aleatoria los ejemplos de entrenamiento
ordenentre = randperm(2*nentre);
xentre = xentre(ordenentre,:);
etientre = etientre(ordenentre);
\end{verbatim}

\begin{figure}
\includegraphics [width=\textwidth]{Ejercicio5_01.png}
\caption{Distribución de los patrones de las dos gaussianas bidimensionales.}
\label{fig:ejercicio51}
\end{figure}



\subsection{Ejercicio 5.2}

Implemente el algoritmo de entrenamiento de un perceptron simple y pruebelo con los datos anteriores, grafique los resultados obtenidos.


\subsubsection*{perceptron.m}

\begin{verbatim}
1     function [w, y] = perceptron( X, yd, alfa, tolerancia, iterMax)
2     % [w, y] = perceptron( X, yd, alfa, tolerancia, iterMax) entrena un 
3     % perceptron a partir de los patrones X y sus clases conocidas yd.
4     %   X son las caracteristicas, cada ejemplo en un renglon
5     %   yd es la salida deseada, la clase de cada ejemplo
6     %   alfa el coeficiente de aprendizaje
7     %   tolerancia de error admitida durante el entrenamiento
8     %   w es el vector de pesos aprendidos, el primer elemento es del bias, w0
9     %   y es el resultado de la clasificación de los patrones de entrenamiento 
10    if nargin < 3
11        alfa = 0.05; % coef de aprendizaje
12    end
13    if nargin < 4
14        tolerancia = 0.1; % tolerancia en el error de entrenamiento
15    end
16    if nargin < 5
17        iterMax = 100; % tolerancia en el error de entrenamiento
18    end
19    
20    N = size(X,1);          % cantidad de patrones de entrenamiento
21    nCarac = size(X,2);     % cantidad de caracteristicas
22    
23    Xbias = [ones(N,1), X]; % agrego unos para entrenar bias
24    y = zeros(size(yd));    % clasificacion de los patrones de entrenamiento
25    
26    w = 0.5*rand(1,nCarac+1); % inicializo los pesos, incluido el del bias
27    error = 1;
28    iter = 0;
29    
30    while (error > tolerancia) && (iter < iterMax)
31        iter = iter + 1;
32        for n = 1:N
33            % salida del perceptron para el patron X(j)
34            y(n) = sign( dot( w, Xbias(n,:) ) );
35            w = w + alfa * (yd(n) - y(n)) * Xbias(n,:); % ajuste de w
36        end
37        error = norm(yd-y) / N;
38    end
39    
40    % Actualizo la clasificacion para los patrones de entrenamiento
41    y = sign(Xbias * w');
42    end
\end{verbatim}
    
\begin{verbatim}
aprendizaje = 0.001; % coeficiente de aprendizaje
toler = 0.01;        % error tolerado en el entrenamiento
% entrenamiento
w = perceptron(xentre,etientre,aprendizaje,toler);
% testeo
etipercep = sign([ones(2*ntest,1) xtest] * w');

x1test = xtest(etipercep == eti1(1),:);%patrones identificados como clase 1
x2test = xtest(etipercep == eti2(1),:);%patrones identificados como clase 2

figure
hold on
plot(x1test(:,1), x1test(:,2),'+b')
plot(x2test(:,1), x2test(:,2),'or')
plot(xtest(etipercep~=etitest,1), xtest(etipercep~=etitest,2),'.k')
plot([1.5 4],-(w(1)+w(2)*[1.5 4])./w(3),'k-')
legend('clase 1','clase 2','errores')
title('Clasificación con Perceptron Simple')
xlabel('x_1'); ylabel('x_2');
axis equal
hold off

errorpercep = sum(etipercep~=etitest) / ntest;
\end{verbatim}


\begin{figure}
\includegraphics [width=\textwidth]{Ejercicio5_02.png}
\caption{Patrones de testeo clasificados con el perceptrón.}
\label{fig:ejercicio52}
\end{figure}


\subsection{Ejercicio 5.3}


Implemente el algoritmo de aprendizaje no supervisado k-medias (batch) y pruebelo con los datos anteriores, grafique los resultados obtenidos.

\subsubsection*{kmedias.m}

\begin{verbatim}
1     function [y, med] = kmedias( X, k, tolerancia, iterMax)
2     %[y, med] = kmedias( X, k, tolerancia, iterMax) agrupa los patrones X en k
3     %grupos. Devuelve la media de los grupos en med y el grupo asignado a cada
4     %patron en y.
5     %   X contiene los patrones, uno en cada renglon
6     %   k define la cantidad de grupos a utilizar
7     %   y grupo asignado a cada patron
8     %   med media de cada grupo por renglon
9     %   tolerancia para definir la convergencia de las medias
10    %   iterMax limita los ciclos posibles para ajustar la media
11    if nargin < 3
12        tolerancia = 0.001; % tolerancia en la convergencia de las medias
13    end
14    if nargin < 4
15        iterMax = 100; % tolerancia en el error de entrenamiento
16    end
17    
18    N = size(X,1);          % cantidad de patrones
19    nCarac = size(X,2);     % cantidad de caracteristicas
20    med = zeros(k,nCarac);  % donde guardo las medias de los k grupos
21    y = zeros(N,1);         % grupo asignado a cada patron
22    grupo = cell(k,1);      % lista los patrones de cada grupo
23    
24    % inicialización, asigno grupos al azar y en igual cantidad
25    indices = randperm(N); % orden arbitrario de los indices
26    Nini = floor(N/k); % cantidad de patrones en cada grupo
27    
28    iter = 0;
29    cambio = 100;
30    while (iter < iterMax) && cambio > tolerancia
31        iter = iter +1;
32        
33        % asignación de grupos
34        if iter == 1
35            % en la iteración inicial asigno arbitrariamente
36            for g = 1:k
37                if g == k
38                    grupo{g} = indices(1+Nini*(k-1):end);
39                else
40                    grupo{g} = indices(1+Nini*(g-1):Nini*g);
41                end
42            end
43        else
44            % en las restantes iteraciones asigno cada patron al centroide mas 
45            % cercano
46            grupo = cell(k,1); % olvido los grupos del paso previo
47            for n = 1:N
48                for g = 1:k
49                    distancias(g) = norm( X(n,:) - med(g,:) );
50                end
51                [~, gmin] = min(distancias);
52                grupo{gmin} = [grupo{gmin} n];
53            end
54        end
55        
56        % actualización de medias
57        medAnteriores = med; % reservo las medias anteriores
58        for g = 1:k
59            med(g,:) = mean(X(grupo{g},:),1); % nuevas medias
60            y(grupo{g})=g; % asignaciones de cada patron a cada grupo
61        end
62        cambio = norm(med - medAnteriores); % cambio de posición de las medias
63    end
64    
65    end
\end{verbatim}
\begin{verbatim}
[etikmed, med] = kmedias(xentre,2);

xAentre = xentre(etikmed == 1,:); %patrones asignados al grupo A
xBentre = xentre(etikmed == 2,:); %patrones asignados al grupo B

figure
hold on
plot(xAentre(:,1), xAentre(:,2),'xb')
plot(xBentre(:,1), xBentre(:,2),'dr')
plot(med(1,1),med(1,2),'*k')
plot(med(2,1),med(2,2),'*k')
legend('grupo A','grupo B','medias')
title('Agrupamiento con k-medias')
xlabel('x_1'); ylabel('x_2');
axis equal
hold off
\end{verbatim}

\begin{figure}
\includegraphics [width=\textwidth]{Ejercicio5_03.png}
\caption{Patrones de entrenamiento agrupados con k-medias.}
\label{fig:ejercicio53}
\end{figure}


\subsection{Ejercicio 5.4}


Implemente el algoritmo de aprendizaje de las redes neuronales con funciones de base radial y pruébelo con los datos anteriores, grafique los resultados obtenidos.


\subsubsection*{redfuncbaserad.m}
\begin{verbatim}
1     function [w, y, mu] = redfuncbaserad( X, yd, alfa, tolerancia, iterMax)
2     %[w, y] = redfuncbaserad( X, yd, alfa, tolerancia, iterMax) entrena una red
3     %neuronal con funciones de base radial.
4     %   X es una matriz de patrones donde cada uno se ubica en un renglon
5     %   yd es la salida deseada, la clase de cada ejemplo
6     %   alfa el coeficiente de aprendizaje
7     %   tolerancia de error admitida durante el entrenamiento
8     %   w es el vector de pesos aprendidos, el primer elemento es del bias, w0
9     %   y es el resultado de la clasificación de los patrones de entrenamiento
10    %   la cantidad de funciones utilizadas es igual a la de clases presentes
11    %   en yd
12    if nargin < 3
13        alfa = 0.05; % coef de aprendizaje
14    end
15    if nargin < 4
16        tolerancia = 0.1; % tolerancia en el error de entrenamiento
17    end
18    if nargin < 5
19        iterMax = 100; % tolerancia en el error de entrenamiento
20    end
21    
22    N = size(X,1);       % cantidad de patrones de entrenamiento
23    nCarac = size(X,2);  % cantidad de caracteristicas
24    y = zeros(size(yd)); % clasificacion de los patrones de entrenamiento
25    
26    % inicialización de las medias de las gaussianas con k-medias
27    k = length(unique(yd)); % k igual a la cantidad de clases
28    [~, mu] = kmedias(X,k); % utilizo las medias
29    
30    % considerando que las funciones de base radial son gaussianas con matriz
31    % de covarianza identidad, se definen como:
32    phi = @(xn,muj) exp(-0.5 * norm( xn-muj ).^2);
33    
34    % de cada patron obtengo nuevas características por medio de las gaussianas
35    Xnuevas = zeros(N,k); % nuevas caracteristicas
36    for n = 1:N
37        for j = 1:k
38            Xnuevas(n,j) = phi(X(n,:),mu(j,:));
39        end
40    end
41    
42    % ahora entreno un perceptron simple con las nuevas caracteristicas
43    w = perceptron(Xnuevas,yd,alfa,tolerancia,iterMax);
44    
45    % clasifico los patrones de entrenamiento
46    y = sign([ones(N,1) Xnuevas] * w');
47    
48    end
49    

\end{verbatim}
    
\subsubsection*{funcbr.m}
\begin{verbatim}
1     function p = funcbr( X, mu )
2     % p(n,j)=fbr(X,mu)= exp(-0.5 * norm( xn-muj ).^2);
3     p = zeros(size(X,1),size(mu,1));
4     
5     phi = @(xn,muj) exp(-0.5 * norm( xn-muj ).^2);
6     for n = 1:size(X,1)
7         for j = 1:size(mu,1)
8             p(n,j) = phi(X(n,:),mu(j,:));
9         end
10    end
11    
12    end
\end{verbatim}

\begin{verbatim}
aprendizaje = 0.005; % coeficiente de aprendizaje
toler = 0.01;        % error tolerado en el entrenamiento
% entrenamiento
[w,~,mu] = redfuncbaserad(xentre,etientre,aprendizaje,toler);
% testeo
etirnfbr = sign([ones(2*ntest,1) funcbr(xtest,mu)] * w');

x1test = xtest(etirnfbr == eti1(1),:);%patrones identificados como clase 1
x2test = xtest(etirnfbr == eti2(1),:);%patrones identificados como clase 2

figure
hold on
plot(x1test(:,1), x1test(:,2),'+b')
plot(x2test(:,1), x2test(:,2),'or')
plot(xtest(etirnfbr~=etitest,1), xtest(etirnfbr~=etitest,2),'.k')
% plot([1 4],-(w(1)+w(2)*[1 4])./w(3),'k-')
legend('clase 1','clase 2','errores')
title('Clasificación con Red Neuronal de Funciones de Base Radial')
xlabel('x_1'); ylabel('x_2');
axis equal
hold off

errorrnfbr = sum(etirnfbr~=etitest) / ntest;
\end{verbatim}

\begin{figure}
\includegraphics [width=\textwidth]{Ejercicio5_04.png}
\caption{Patrones de testeo clasificados con la red neuronal de funciones de base radial.}
\label{fig:ejercicio54}
\end{figure}


\subsection{Ejercicio 5.5}

Implemente el algoritmo de los k-vecinos más cercanos y pruebelo con los datos anteriores, grafique los resultados obtenidos.

\subsubsection*{kvecinos.m}
\begin{verbatim}
1     function Cdesc = kvecinos( Xdesc, k, Xcon, Ccon )
2     %Cdesc = kvecinos( Xdesc, k, Xcon, Ccon ) clasifica los patrones
3     %desconocidos a partir de los k vecinos más cercanos conocidos.
4     %   Xdesc son los patrones a clasificar, cada uno en un renglon
5     %   k es la cantidad de vecinos a buscar para asignar la clase
6     %   Xcon son los patrones de clase conocida dada en Ccon
7     %   Xcon tiene cada patron por renglon, y cada elemento de Ccon está
8     %   asociado al patrón de Xcon que se ubicado en igual posición.
9     
10    Ndesc = size(Xdesc,1);  % cantidad de patrones desconocidos
11    Ncon = size(Xcon,1);    % cantidad de patrones conocidos
12    Cdesc = zeros(Ndesc,1); % clases asignadas a los patrones no vistos
13    
14    distancias = zeros(Ncon,1);
15    % A cada patron no visto le asigno su clase
16    for nd = 1:Ndesc
17        % Se calcula la distancia contra todos los conocidos
18        for nc = 1:Ncon
19            distancias(nc) = norm( Xcon(nc,:) - Xdesc(nd,:) );
20        end
21        
22        % se ordenan las distancias de menor a mayor
23        [~, orden] = sort(distancias); 
24        
25        % se listan las clases de los k vecinos mas cercanos
26        kvecclases = Ccon(orden(1:k)); % clases de cada uno de los k vecinos
27        clases = unique(kvecclases);    % clases posibles entre los k vecinos
28        cuenta = zeros(size(clases));  % cuantos patrones hay de cada clase?
29        for c = 1:length(clases)
30            cuenta(c) = sum(clases(c) == kvecclases);
31        end
32        [~, cmax] = max(cuenta); % clase con mayor presencia entre los k
33        
34        Cdesc(nd) = clases(cmax);% asignación de clase al patron desconocido n
35    end
36    
37    end
\end{verbatim}

\begin{verbatim}
k = 10;
etikvec = kvecinos(xtest,k,xentre, etientre);

x1test = xtest(etikvec == eti1(1),:);
x2test = xtest(etikvec == eti2(1),:);

figure
hold on
plot(x1test(:,1), x1test(:,2),'+b')
plot(x2test(:,1), x2test(:,2),'or')
plot(xtest(etikvec~=etitest,1), xtest(etikvec~=etitest,2),'.k')
legend('clase 1','clase 2','errores')
title(['Clasificación con k-vecinos más cercanos (k=' num2str(k) ')'])
xlabel('x_1'); ylabel('x_2');
axis equal
hold off

errorkvec = sum(etikvec~=etitest) / ntest;
\end{verbatim}

\begin{figure}
\includegraphics [width=\textwidth]{Ejercicio5_05.png}
\caption{Patrones de testeo clasificados con k-vecinos más cercanos, $k=10$.}
\label{fig:ejercicio55}
\end{figure}


\subsection{Ejercicio 5.6.}


Compare y comente los resultados obtenidos por los distintos métodos.\\


Considerando los mismos grupos de testeo y entrenamiento los resultados durante el testeo fueron los siguientes:

\begin{verbatim}Tasa de error para el perceptron: 8.33 %
Tasa de error para la rn-fbr: 20.00 %
Tasa de error para el k-vecinos: 8.33 %
\end{verbatim}
    
El agrupamiento resultante de aplicar k medias resultó bastante similar a las distribuciones originales. Una probable causa es que las medias se ubicaron cercanas a las medias de las distribuciones originales. Otra causa es que el grado de solapamiento es bajo y las distribuciones de las clases no tienen más de una moda. También ayudó el hecho de haber utilizado igual cantidad de grupos que como clases había. Aquí se ve justificada la efectividad de utilizar k-medias en el primer paso del entrenamiento de las redes neuronales con funciones de base radial (rnfbr).

Al analizar los algoritmos de clasificación supervisados vemos que el perceptrón y k-vecinos más cercanos obtuvieron el menor error, apenas superior al 8 \%. En ambos casos los patrones mal clasificados se encuentran en la zona de solapamiento entre ambas clases, como es de esperar. En cambio la rnfbr tuvo un error bastante superior, del 20 \%. Esto se debe a que las covarianzas se consideraron matrices identidades, lo que claramente se observa en la Figura \ref{fig:ejercicio54} dado que se puede completar un círculo que encierre a la clase 1; también porque hay unos patrones mal identificados como de clase 2 cerca del origen, lo que indica que los pesos aplicados a cada función de base radial son opuestos.









\clearpage

\section{Ejercicio 6 - Redes neuronales dinámicas}

\subsection{Ejercicio 6.1}

Implemente la arquitectura y entrenamiento Hebbiano para una red recurrente de Hopfield con los patrones que se muestran en la Figura 1.

\begin{verbatim}
load('patrones5x5.mat') % cargo los patrones desde un archivo

% Grafico los 3 patrones de la figura 1

N = size(patrones5x5,2);           % cantidad de patrones

invgray = flipud(colormap(gray));  % escala de colores a utilizar
colormap(invgray);                 % blanco y negro

for p = 1:3
    patronReshaped = reshape(patrones5x5(p,:),[5 5]); % patron --> imagen
    subplot(3,3,p); imagesc(patronReshaped'); axis equal tight
    title('Patron original')
end
\end{verbatim}

Dados los 3 patrones (ver Figura \ref{fig:ejercicio62}) entreno con ellos una red de Hopfield y obtengo la matriz de pesos.

\begin{verbatim}
W = entrenarHopfield(patrones5x5); % matriz de pesos de la red

Pmax = N / (2*log(N));             % capacidad maxima de almacenamiento

fprintf(['La capacidad de almacenamiento %0.2f ' ...
         'supera la cantidad de patrones a recordar (3).\n'], Pmax);
\end{verbatim}

\begin{verbatim}La capacidad de almacenamiento 3.88 supera la cantidad de patrones a recordar (3).
\end{verbatim}
    
Ahora pruebo recuperar los patrones originales a partir de patrones ruidosos (fila central Figura \ref{fig:ejercicio62}).

\begin{verbatim}
cambiar = randperm(N,4); % 4 posiciones que voy a cambiar de signo

for p = 1:3
    ruidoso = patrones5x5(p,:);
    ruidoso(cambiar) = (-1)*ruidoso(cambiar);
    recup = recuperarHopfield(ruidoso, W); % recuperado
    recupReshaped = reshape(recup,[5 5]); % patron --> imagen
    subplot(3,3,6+p); imagesc(recupReshaped'); axis equal tight off
    title('Patron recuperado')
    subplot(3,3,3+p); imagesc(reshape(ruidoso,[5 5])'); axis equal tight off
    title('Patron ruidoso')
end
\end{verbatim}

\begin{figure}
\includegraphics [width=\textwidth]{Ejercicio6_02.png}
\caption{Patrones originales (fila superior), patrones ruidosos (fila central) y  patrones recuperados por la red (fila inferior).}
\label{fig:ejercicio62}
\end{figure}

Con el nivel de ruido introducido, 4 cambios en 25 elementos, el segundo y tercer patrón son recuperados en la mayoría de las oprtunidades. En cambio el primer patrón es recuperado con un error en el bit central.

El código de las funciones utilizadas se transcribe a continuación.

\subsubsection*{entrenarHopfield.m}

\begin{verbatim}
1     function  W = entrenarHopfield( X )
2     % W = entrenarHopfield( X )
3     %   X es una matriz donde cada renglón es un patrón que se desea almacenar.
4     %   W es la matriz de pesos de la red de Hopfield. Dado un patrón, recuerda
5     %     cuales elementos se [des]activaron simultáneamente. 
6     
7     N = size(X,2); % Longitud de cada patron
8     m = size(X,1); % Cantidad de patrones
9     
10    % Entrenamiento Hebbiano
11    % W = x1' * x1 + x2' * x2 + ... + xm' * xm - m * I
12    % La expresión anterior es euivalente a la siguiente
13    W = X' * X - m * eye(N);
14    
15    end
\end{verbatim}
    

\subsubsection*{recuperarHopfield.m}

\begin{verbatim}
1     function y = recuperarHopfield( y0, W, iterMax, graficar)
2     % y = recuperarHopfield( y0, W, iterMax, graficar)
3     %   y0 es el patrón dado a la entrada, a partir del cuál se recupera y.
4     %      Debe ser un vector fila. 
5     %   W es la matriz de pesos de la red de Hopfield.
6     %   iterMax son las iteraciones máximas permitidas, por defecto son 100.
7     %   graficar es un valor lógico que grafica cada estado intermedio. Por 
8     %      defecto es falso.
9     if nargin < 3
10        iterMax = 100; % iteraciones máximas
11    end
12    if nargin < 4
13        graficar = 0;  % deshabilita la graficación paso por paso
14    end
15    
16    N = length(y0); %
17    iter = 0;       % contador de iteraciones
18    y = y0;         % se inicializa el patrón recuperado igual al patrón dado
19    yant = -y;      % asegura entrar al while sin modificar el algoritmo
20    
21    % Gráfica del patrón de entrada
22    if graficar
23        invgray = flipud(colormap(gray));
24        figure; colormap(invgray)
25        imagesc(reshape(y,[5 5])'); axis equal; axis tight;
26        pause
27    end
28    
29    % El algoritmo itera hasta que el patrón recuperado cumple con y = y * W
30    while (iter < iterMax) && ~isequal(yant, y) 
31        iter = iter + 1;        % contador de iteraciones
32        yant = y;               % se guarda el patrón de la iteración anterior
33        y = sign(yant * W);     % se obtiene el nuevo patrón
34        yEnCero = find(y == 0); % posiciones dónde y se hace cero
35        y(yEnCero) = yant(yEnCero); % si y es 0 se mantiene el anterior valor 
36        
37        % Gráfica del patrón recuperado hasta esta iteración
38        if graficar
39            imagesc(reshape(y,[5 5])'); axis equal; axis tight;
40            pause
41        end
42    end
43    
44    end
\end{verbatim}
    

\subsection{Ejercicio 6.2}

Utilice la red de Hopfield como memoria asociativa para los patrones de la figura 2.

\begin{verbatim}
load('numeros7x5.mat') % en la posicion 10 se ubica el 0

numMax = 4; % limite de la cantidad de digitos a almacenar
numeros7x5 = numeros7x5(1:numMax,:);% carga los digitos indicados
N = size(numeros7x5,2);             % cantidad de pixeles

invgray = flipud(colormap(gray));  % escala de colores a utilizar
colormap(invgray);                 % blanco y negro

for p = 1:numMax % 10 es 0
    patronReshaped = reshape(numeros7x5(p,:),[7 5]); % patron --> imagen
    subplot(4,numMax,p); imagesc(patronReshaped);
    axis equal tight off
    title('Patron original')
end
\end{verbatim}

Dados los numMax patrones entreno con ellos una red de Hopfield y obtengo la matriz de pesos.

\begin{verbatim}
W = entrenarHopfield(numeros7x5); % matriz de pesos de la red

Pmax = N / (2*log(N));             % capacidad maxima de almacenamiento

fprintf(['La capacidad de almacenamiento es de %0.2f.\n'], Pmax);
\end{verbatim}

\begin{verbatim}La capacidad de almacenamiento es de 4.92.
\end{verbatim}
    

\subsection{Ejercicio 6.3}

Agregue diferentes cantidades de ruido a los patrones de la Figura 2 y utilice estos ejemplos ruidosos para acceder a las memorias fundamentales almacenadas como en el ejercicio anterior. Para simular cantidades controladas de ruido se sugiere invertir (blanco a negro y viceversa) cada pixel del dígito con probabilidades 0.1, 0.2 y 0.5.


Ahora pruebo recuperar los patrones originales a partir de los patrones con distintos niveles de ruido.

\begin{verbatim}
ruidos = [0.1 0.2 0.5]; % niveles de ruido

for p = 1:numMax
    for r = 1:3
        nCambios = floor(N*ruidos(r));
        cambiar = randperm(N,nCambios); % posiciones a cambiar de signo
        ruidoso = numeros7x5(p,:);
        ruidoso(cambiar) = (-1)*ruidoso(cambiar);
        recup = recuperarHopfield(ruidoso, W); % recuperado
        recupReshaped = reshape(recup,[7 5]); % patron --> imagen
        subplot(4,numMax,r*numMax+p); imagesc(recupReshaped);
        axis off equal tight
        tit = sprintf('%2.0f%% ruido',100*nCambios/N);
        title(tit)
    end
end
\end{verbatim}

\begin{figure}
\includegraphics [width=\textwidth]{Ejercicio6_04.png}
\caption{Patrones originales (fila superior) y patrones recuperados (filas restantes) a partir de patrones ruidosos con distintos niveles de ruido.}
\label{fig:ejercicio64}
\end{figure}

Al entrenar con los 10 números se sobrepasa la capacidad de almacenamiento máxima de la red (4,92) por lo que se recupera siempre el mismo dígito (9) o su versión especular, aún en los niveles de ruidos más bajos.

Reduciendo la cantidad de dígitos en la memoria se observa que para cuatro dígitos o menos se logra obtener cierto grado de recuperación. Si se entrena con cuatro dígitos (como se observa en la Figura \ref{fig:ejercicio64}) se logra recuperar el 1 y el 4, en cambio el 2 y el 3 al ser parecidos devuelven un patrón que se parece a ambos pero no es ninguno de ellos. Con el mayor nivel de ruido (50\%) se pierde toda posibilidad de recuperación.

\clearpage






\section{Ejercicio 7 - Markov Completar!!!}





\clearpage


\section{Ejercicio 8 - Árboles de decisión}


\subsection{Ejercicio 8.1}

Se tienen datos acerca de la realización o suspensión de partidos de tenis en función del pronóstico del tiempo y los datos del día que se han volcado en el Cuadro 1:

\begin{verbatim}
% atributos de cada ejemplo: pronostico, temperatura, humedad, viento
% el ulitmo elemento de cada ejemplo es la clase
datosCuadro1 = {'soleado',  'calor',    'alta',     'no', 'N';
                'soleado',  'calor',    'alta',     'si', 'N';
                'nublado',  'calor',    'alta',     'no', 'P';
                'lluvioso', 'moderado', 'alta',     'no', 'P';
                'lluvioso', 'frio',     'normal',   'no', 'P';
                'lluvioso', 'frio',     'normal',   'si', 'N';
                'nublado',  'frio',     'normal',   'si', 'P';
                'soleado',  'moderado', 'alta',     'no', 'N';
                'soleado',  'frio',     'normal',   'no', 'P';
                'lluvioso', 'moderado', 'normal',   'no', 'P';
                'soleado',  'moderado', 'normal',   'si', 'P';
                'nublado',  'moderado', 'alta',     'si', 'P';
                'nublado',  'calor',    'normal',   'no', 'P';
                'lluvioso', 'moderado', 'alta',     'si', 'N';};
\end{verbatim}

\begin{itemize}
   \item[a)] Construya ``a mano'' (haciendo todas las cuentas) y dibuje el árbol binario de decisión que describa los datos sobre juegos de tenis del cuadro. Utilice como impureza la entropía.
\end{itemize}

El árbol construido a mano se esquematiza en las páginas adjuntas del anexo \ref{anexo}. 
Los cálculos realizados a mano se incluyen a continuación del esquema del árbol.

\begin{itemize}

   \item[b)] Escriba un programa que permita crecer árboles de decisión binarios y utilícelo para generar automáticamente un árbol a partir de los datos anteriores. Contraste con el árbol generado por Ud.
\end{itemize}


\subsubsection*{crecerArbol.m}

\begin{verbatim}
1     function arbol = crecerArbol(datos, ejemplos, listaCaracteristicas,...
2                                  tipoImpureza, debugging)
3     % arbol = crecerArbol(datos,ejemplos,listaCaracteristicas,...
                            tipoImpureza, debugging)
4     % construye el arbol a partir de los datos dados
5     %   datos es una celda donde cada ejemplo esta por renglon y por columnas
6     %   las características. La ultima columna corresponde a la clase. Las
7     %   caracteristicas pueden ser cadenas de caracteres o numéricas.
8     %   
9     %   ejemplos es una lista de los ejemplos a utilizar, por defecto se
10    %   utilizan todos si ejemplos es una matriz vacía.
11    %   
12    %   listaCaracterísticas es una celda con los nombres de cada tipo de
13    %   característica
14    %
15    %   tipoImpureza es un string que configura la impureza a utilizar, puede
16    %   ser:
17    %   - 'ent': calcula la impureza por la entropia (por defecto)
18    %   - 'gini': calcula la impureza de Gini
19    %   - 'miss': impureza por la minima probabilidad de errar la clasificación
20    %
21    %   debugging imprime informacion por pantalla:
22    %   - 0: no imprime nada
23    %   - 1: imprime la información de cada nodo del arbol generado
24    %   - 2: igual que 1 más información intermedia utilizada en el crecimiento
25    %        del árbol
26    %
27    %   arbol es una celda donde cada elemento es un nodo. Cada nodo tiene
28    %   distinto tipo de información de acuerdo a su tipo (por ejemplo, id,
29    %   pregunta, hijos, clase)
30    %   clase 
31    if (nargin < 2) || isempty(ejemplos)
32        n1.ejemplos = 1:size(datos,1); % inicializo con todos los ejemplos
33    else
34        n1.ejemplos = ejemplos; % inicializo con los ejemplos dados
35    end
36    if (nargin < 3) || isempty(listaCaracteristicas)
37        auxc = 1:(size(datos,2)-1); % todas las caracteristicas
38        listaCaracteristicas = sprintf('Atributo%i ',auxc);
39        listaCaracteristicas = strsplit(listaCaracteristicas(1:end-1));
40    end
41    if (nargin < 4) || isempty(tipoImpureza)
42        tipoImpureza = 'ent'; % calcula impureza por entropia
43    end
44    if nargin < 5
45        debugging = false; % Por defecto no imprime por pantalla
46    end
47    
48    
49    n1.id = 1;                     % id del nodo raiz
50    nodosARevisar = n1.id;         % el primer nodo a revisar es el nodo raiz
51    n1.tipo = 'aRevisar';          % tipo provisorio
52    arbol{1} = n1;                 % arbol inicial con nodo raiz
53    
54    % mientras queden nodos por revisar
55    while ~isempty(nodosARevisar)
56        % Reviso el siguiente nodo de la lista
57        nodo.id = nodosARevisar(1);
58        % enumero los ejemplos que le corresponden 
59        ejemplos = arbol{nodosARevisar(1)}.ejemplos;
60        
61        % Genera una lista con las clases encontradas en los ejemplos
62        clasesPorEjemplo = datos(ejemplos,end); %lista la clase de cada ejemplo
63        clases = unique(clasesPorEjemplo);      %lista con las posibles clases
64    
65        %% Si todos los ejemplos pertenecen a la misma clase creo un nodo hoja
66        if length(clases) == 1
67            nodo.tipo = 'hoja';             % tipo de nodo
68            nodo.clase = clases{1};         % clase del nodo hoja
69    
70        %% Si no son todos los ejemplos de la misma clase se debe elegir que 
71        %  caracteristica utilizar
72        else
73            % Cantidad de ejemplos e impureza del nodo padre
74            nPadre = length(ejemplos);                 
75            iPadre = impureza(clasesPorEjemplo, tipoImpureza); 
76            deltaimax = 0; %variable auxiliar para buscar la mejor ramificación
77            nodo.tipo = 'sinAsignar'; % etiqueta de control interno
78            
79            % Para cada caracteristica candidata
80            for lc = 1:length(listaCaracteristicas)
81                % si las caracteristicas son numéricas
82                auxcarac = horzcat(datos{ejemplos,lc});
83                esNumerica = isnumeric(auxcarac);
84                
85                % exploro las alternativas de cada caracteristica
86                if esNumerica
87                    % si es numérica genero los candidatos ordenando por valor
88                    % y tomando el valor medio en cada salto de clase
89                    [valor, orden] = sort(horzcat(datos{ejemplos,lc}));
90                    clasesOrdenadas = clasesPorEjemplo(orden);
91                    decisionesCandidatas = [];
92                    for co = 2:length(clasesOrdenadas)
93                        if ~strcmp(clasesOrdenadas(co-1),clasesOrdenadas(co))
94                            vmedio = 0.5 * (valor(co-1) + valor(co));
95                            decisionesCandidatas=[decisionesCandidatas vmedio];
96                        end
97                    end
98                else
99                    % si es categórica cada valor se vuelve candidato
100                   decisionesCandidatas = unique(datos(ejemplos,lc));
101               end
102               iterFinal = length(decisionesCandidatas); % itero sobre todas 
103                                                         % las opciones
104               
105               % si hay dos valores posibles no necesita iterar más de una vez
106               if (iterFinal == 2) && esNumerica
107                   iterFinal = 1;
108               end
109               
110               % Para cada valor posible de la caracteristica candidata 
                    se calcula
111               % el deltai, a la vez que se compara con el maximo almacenado.
112               for dc = 1:iterFinal
113                   if esNumerica
114                       auxHijoSi = (decisionesCandidatas(dc) > auxcarac);
115                   else
116                       auxHijoSi = strcmp(decisionesCandidatas(dc),...
                                             datos(ejemplos,lc));
117                   end
118                   auxHijoNo = ~auxHijoSi;
119                   nHijoSi = sum(auxHijoSi);
120                   nHijoNo = sum(auxHijoNo);
121                   iHijoSi = impureza(clasesPorEjemplo(auxHijoSi), tipoImpureza);
122                   iHijoNo = impureza(clasesPorEjemplo(auxHijoNo), tipoImpureza);
123                   
124                   deltai = iPadre - (nHijoSi/nPadre) * iHijoSi - ...
125                       (nHijoNo/nPadre) * iHijoNo;
126                      
127                   % Si deltai es mayor al maximo pasa a ser la decision elegida
128                   if deltai > deltaimax
129                       deltaimax = deltai;
130                       
131                       % El nodo actual se divide en dos.
132                       nodo.tipo = 'divisor';
133                       % Se guarda la pregunta utilizada para dividir
134                       if esNumerica
135                           nodo.pregunta = ['¿' listaCaracteristicas{lc} ...
136                                            sprintf(' < %0.1f?',...
                                                       decisionesCandidatas(dc))];
137                       else
138                           nodo.pregunta = ['¿' listaCaracteristicas{lc} ...
139                                            '=' decisionesCandidatas{dc} '?'];
140                       end
141                       % Se asignan los id de los nodos que responden si y no
142                       nodo.HijoSi = length(arbol) + 1;
143                       nodo.HijoNo = length(arbol) + 2;
144                       % Tipo provisorio
145                       nodoHijoSi.tipo = 'aRevisar';
146                       nodoHijoNo.tipo = 'aRevisar';
147                       % Los ejemplos del padre se dividen entre los nodos hijos
148                       nodoHijoSi.ejemplos = ejemplos(auxHijoSi);
149                       nodoHijoNo.ejemplos = ejemplos(auxHijoNo);
150                   end
151                   
152                   if debugging > 1
153                       if esNumerica
154                           preg = ['¿' listaCaracteristicas{lc} ...
155                                            sprintf(' < %0.1f?',...
                                                       decisionesCandidatas(dc))];
156                       else
157                           preg = ['¿' listaCaracteristicas{lc} ...
158                                            '=' decisionesCandidatas{dc} '?'];
159                       end
160                       fprintf('pregunta: %s', preg)
161                       fprintf('deltai: %0.5f\n', deltai)
162                   end
163               end
164           end
165       end
166       
167       % Si no se pudo ramificar y el nodo contiene más de una clase
168       if strcmp(nodo.tipo,'sinAsignar')
169           cuentaXclase = histClases(clasesPorEjemplo); % simil histograma
170           [~, imax] = max(cuentaXclase);  % indice de la clase mayoritaria
171           nodo.clase = clases{imax};      % clase mayoritaria
172           nodo.tipo = 'hoja';             % tipo de nodo
173       end
174       
175       % Si está activo el debugging imprimo en pantalla
176       if debugging > 1
177           fprintf('\nEn nodo %i reviso los ejemplos:\n',nodo.id);
178           fprintf('%i, ',ejemplos);
179           
180           if strcmp(nodo.tipo,'hoja')
181               fprintf('\nNodo hoja, clase: %s\n', nodo.clase);
182               if length(clases) > 1
183                   fprintf('------¡Impuro!----------\n')
184                   fprintf('%s\t', clases{:});
185                   fprintf('\n');
186                   fprintf('%i\t', cuentaXclase);
187                   fprintf('\n');
188               end
189           elseif strcmp(nodo.tipo,'divisor')
190               fprintf('\nNodo divisor: %s\n', nodo.pregunta);
191           else
192               disp('---- ¡Ups! el árbol tiene una rama seca ----');
193           end
194       end
195       
196       % Definido el nodo lo agrego al arbol
197       arbol{nodosARevisar(1)} = nodo;
198       
199       % Si es un nodo divisor agrego los hijos a la lista para revisar
200       if strcmp(nodo.tipo, 'divisor')
201           arbol{nodo.HijoSi} = nodoHijoSi;
202           arbol{nodo.HijoNo} = nodoHijoNo;
203           nodosARevisar = [nodosARevisar nodo.HijoSi nodo.HijoNo];
204       end
205       nodosARevisar = setdiff(nodosARevisar, nodosARevisar(1));
206       
207       % Limpieza antes del siguiente ciclo
208       clear nodo nodoHijoSi nodoHijoNo; 
209   end
210   
211   % Si está activo el debugging imprime el árbol creado
212   if debugging
213       disp('-------------------------')
214       disp('Detalles del árbol creado')
215       disp('-------------------------')
216       for aa = 1:length(arbol)
217           disp(arbol{aa})
218       end
219   end
220   
221   end
\end{verbatim}
    

\subsubsection*{impureza.m}

\begin{verbatim}
1     function imp = impureza( clasesEnEjemplos, metodo)
2     % Calcula la impureza del nodo dada la lista de clases por ejemplos. 
3     %   imp = impureza( clasesEnEjemplos, metodo)
4     %   clasesEnEjemplos: lista con la clase de cada ejemplo
5     %   imp: impureza calculada
6     %   metodo: utilizado para calcular la impureza, definido por un string:
7     %   - 'ent': calcula la impureza por la entropia.
8     %   - 'gini': calcula la impureza de Gini
9     %   - 'miss': impureza por la minima probabilidad de errar la clasificación
10    
11    cuentas = histClases(clasesEnEjemplos); % cantidad de ejemplos por clase
12    total = sum(cuentas);                   % cantidad de ejemplos en el nodo
13    imp = 0;                                % inicializo la impureza
14    
15    % Si no son todos de la misma clase
16    if all(cuentas ~= total)
17        if strcmp(metodo, 'ent')
18            imp = - sum( (cuentas./total) .* log(cuentas./total) );
19        elseif strcmp(metodo, 'gini')
20            imp = 1 - sum((cuentas./total).^2);
21        elseif strcmp(metodo, 'miss')
22            imp = 1 - max(cuentas./total);
23        else
24            disp('------ Perdón, no sé calcular esa impureza :( ------');
25        end
26    end
27    
28    end
\end{verbatim}

\begin{verbatim}
% listaEjemplos = 1:14; uso todos, no hace falta
nombresCaract = {'pronostico', 'temperatura', 'humedad', 'viento'};
debug = 1; % solo publica el arbol construido
tipoImpureza = 'ent';
arbol1b = crecerArbol(datosCuadro1, [],nombresCaract,tipoImpureza,debug);
\end{verbatim}

\begin{verbatim}-----------------------------------------------------------
Detalles del árbol creado (datosCuadro1, impureza=entropia)
-----------------------------------------------------------
          id: 1
        tipo: 'divisor'
    pregunta: '¿pronostico=nublado?'
      HijoSi: 2
      HijoNo: 3

       id: 2
     tipo: 'hoja'
    clase: 'P'

          id: 3
        tipo: 'divisor'
    pregunta: '¿humedad=alta?'
      HijoSi: 4
      HijoNo: 5

          id: 4
        tipo: 'divisor'
    pregunta: '¿pronostico=lluvioso?'
      HijoSi: 6
      HijoNo: 7

          id: 5
        tipo: 'divisor'
    pregunta: '¿viento=no?'
      HijoSi: 8
      HijoNo: 9

          id: 6
        tipo: 'divisor'
    pregunta: '¿viento=no?'
      HijoSi: 10
      HijoNo: 11

       id: 7
     tipo: 'hoja'
    clase: 'N'

       id: 8
     tipo: 'hoja'
    clase: 'P'

          id: 9
        tipo: 'divisor'
    pregunta: '¿pronostico=lluvioso?'
      HijoSi: 12
      HijoNo: 13

       id: 10
     tipo: 'hoja'
    clase: 'P'

       id: 11
     tipo: 'hoja'
    clase: 'N'

       id: 12
     tipo: 'hoja'
    clase: 'N'

       id: 13
     tipo: 'hoja'
    clase: 'P'
\end{verbatim}
    
Este árbol da igual que el que hice a mano, sólo cambia la forma de expresar las preguntas. Por ejemplo, en nodo 4 (D) en vez de preguntar por ¿soleado? pregunta por ¿lluvioso? que es equivalente porque hay dos posibilidades en ese momento (ya se había preguntado por nublado). En nodo 9 (I) donde podría haber elegido temperatura también elige pronóstico

El árbol construido (datosCuadro1, impureza=entropía) con la función crecerArbol() se esquematiza en las páginas adjuntas del anexo \ref{anexo}. 


\begin{itemize}
   \item[c)] Modifique el tipo de impureza empleada y comente las diferencias con el árbol generado en el punto anterior.
\end{itemize}
\begin{verbatim}
tipoImpureza = 'gini';
arbol1cgini = crecerArbol(datosCuadro1, [],nombresCaract,tipoImpureza,debug);
\end{verbatim}

\begin{verbatim}-------------------------------------------------------
Detalles del árbol creado (datosCuadro1, impureza=gini)
-------------------------------------------------------
          id: 1
        tipo: 'divisor'
    pregunta: '¿pronostico=nublado?'
      HijoSi: 2
      HijoNo: 3

       id: 2
     tipo: 'hoja'
    clase: 'P'

          id: 3
        tipo: 'divisor'
    pregunta: '¿humedad=alta?'
      HijoSi: 4
      HijoNo: 5

          id: 4
        tipo: 'divisor'
    pregunta: '¿pronostico=lluvioso?'
      HijoSi: 6
      HijoNo: 7

          id: 5
        tipo: 'divisor'
    pregunta: '¿viento=no?'
      HijoSi: 8
      HijoNo: 9

          id: 6
        tipo: 'divisor'
    pregunta: '¿viento=no?'
      HijoSi: 10
      HijoNo: 11

       id: 7
     tipo: 'hoja'
    clase: 'N'

       id: 8
     tipo: 'hoja'
    clase: 'P'

          id: 9
        tipo: 'divisor'
    pregunta: '¿pronostico=lluvioso?'
      HijoSi: 12
      HijoNo: 13

       id: 10
     tipo: 'hoja'
    clase: 'P'

       id: 11
     tipo: 'hoja'
    clase: 'N'

       id: 12
     tipo: 'hoja'
    clase: 'N'

       id: 13
     tipo: 'hoja'
    clase: 'P'
\end{verbatim}
    
El árbol calculado con la impureza de gini es igual que el hecho a mano y que aquel del punto 1.b.
No lo vuelvo a graficar.

\begin{verbatim}
tipoImpureza = 'miss';
arbol1cmiss = crecerArbol(datosCuadro1, [],nombresCaract,tipoImpureza,debug);
\end{verbatim}

\begin{verbatim}-------------------------
Detalles del árbol creado (datosCuadro1, impureza=miss)
-------------------------
          id: 1
        tipo: 'divisor'
    pregunta: '¿pronostico=soleado?'
      HijoSi: 2
      HijoNo: 3

          id: 2
        tipo: 'divisor'
    pregunta: '¿humedad=alta?'
      HijoSi: 4
      HijoNo: 5

          id: 3
        tipo: 'divisor'
    pregunta: '¿temperatura=frio?'
      HijoSi: 6
      HijoNo: 7

       id: 4
     tipo: 'hoja'
    clase: 'N'

       id: 5
     tipo: 'hoja'
    clase: 'P'

          id: 6
        tipo: 'divisor'
    pregunta: '¿pronostico=lluvioso?'
      HijoSi: 8
      HijoNo: 9

       id: 7
     tipo: 'hoja'
    clase: 'P'

          id: 8
        tipo: 'divisor'
    pregunta: '¿viento=no?'
      HijoSi: 10
      HijoNo: 11

       id: 9
     tipo: 'hoja'
    clase: 'P'

       id: 10
     tipo: 'hoja'
    clase: 'P'

       id: 11
     tipo: 'hoja'
    clase: 'N'
\end{verbatim}
    
El árbol calculado con la impureza de clasificar mal difiere de los anteriores desde la primera pregunta. Tiene dos nodos menos y tiene una hoja impura (nodo 7), al contrario de los anteriores los grupos que se formaron al clasificar los ejemplos son distintos en su mayoría.\\

El árbol construido (datosCuadro1, impureza=missclasification) con la función crecerArbol() se esquematiza en las páginas adjuntas del anexo \ref{anexo}.



\subsection*{Ejercicio 8.2}


Se han modificado los datos de juegos de tenis anteriores para incluir valores numéricos de temperatura y humedad de acuerdo con el Cuadro 2: Modifique el programa desarrollado para tratar con atributos numéricos y genere un nuevo árbol con los datos del cuadro.

\begin{verbatim}
datosCuadro2 = {'soleado',  85,    85,   'no', 'N';
                'soleado',  80,    90,   'si', 'N';
                'nublado',  83,    78,   'no', 'P';
                'lluvioso', 70,    96,   'no', 'P';
                'lluvioso', 68,    80,   'no', 'P';
                'lluvioso', 65,    70,   'si', 'N';
                'nublado',  64,    65,   'si', 'P';
                'soleado',  72,    95,   'no', 'N';
                'soleado',  69,    70,   'no', 'P';
                'lluvioso', 75,    80,   'no', 'P';
                'soleado',  75,    70,   'si', 'P';
                'nublado',  72,    90,   'si', 'P';
                'nublado',  81,    75,   'no', 'P';
                'lluvioso', 71,    80,   'si', 'N';};


tipoImpureza = 'ent';
arbol2 = crecerArbol(datosCuadro2, [],nombresCaract,tipoImpureza,debug);
\end{verbatim}

\begin{verbatim}-------------------------
Detalles del árbol creado (datosCuadro2, impureza=entropia)
-------------------------
          id: 1
        tipo: 'divisor'
    pregunta: '¿pronostico=nublado?'
      HijoSi: 2
      HijoNo: 3

       id: 2
     tipo: 'hoja'
    clase: 'P'

          id: 3
        tipo: 'divisor'
    pregunta: '¿temperatura < 77.5?'
      HijoSi: 4
      HijoNo: 5

          id: 4
        tipo: 'divisor'
    pregunta: '¿temperatura < 73.5?'
      HijoSi: 6
      HijoNo: 7

       id: 5
     tipo: 'hoja'
    clase: 'N'

          id: 6
        tipo: 'divisor'
    pregunta: '¿viento=no?'
      HijoSi: 8
      HijoNo: 9

       id: 7
     tipo: 'hoja'
    clase: 'P'

          id: 8
        tipo: 'divisor'
    pregunta: '¿temperatura < 71.0?'
      HijoSi: 10
      HijoNo: 11

       id: 9
     tipo: 'hoja'
    clase: 'N'

       id: 10
     tipo: 'hoja'
    clase: 'P'

       id: 11
     tipo: 'hoja'
    clase: 'N'
\end{verbatim}
    
La función presentada anteriormente ya contiene los cambios realizados para tratar con atributos numéricos. No repito aquí el código.\\

En este caso no usa el pronóstico más allá del nodo raíz. La humedad no la usa nunca. La temperatura la usa 3 veces, aprovechando que tiene más cortes candidatos. El viento una sola vez. El árbol es más profundo en este caso puede ser que se esmere por ir separanado en hojas y termine utilizando más decisiones de las necesarias, típico de búsqueda en profundidad. Es más desbalanceado que en los casos anteriores, quizá por tener más flexibilidad de elección prefiere ir dejando nodos hojas puros en el camino.

El árbol construido (datosCuadro2, impureza=entropia) con la función crecerArbol() se esquematiza en las páginas adjuntas del anexo \ref{anexo}.

\clearpage
\appendix
\section{Anexo árboles de decisión}
\label{anexo}






\end{document}